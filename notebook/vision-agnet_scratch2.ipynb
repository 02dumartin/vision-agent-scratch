{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71bdc5fe",
   "metadata": {},
   "source": [
    "Rough한 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe1bdf6",
   "metadata": {},
   "source": [
    "Utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf489a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic module\n",
    "import os, sys, copy\n",
    "\n",
    "# data handling / io\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, base64\n",
    "\n",
    "# image handling\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "# LLM\n",
    "from openai import OpenAI\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Utils\n",
    "from tqdm import tqdm\n",
    "import time, argparse, logging, uuid\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc5ba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project root \n",
    "def find_project_root(marker_filename=\".project-root\"):\n",
    "    current_dir = os.path.abspath(os.getcwd())\n",
    "    while True:\n",
    "        if os.path.isfile(os.path.join(current_dir, marker_filename)):\n",
    "            return current_dir\n",
    "        parent_dir = os.path.dirname(current_dir)\n",
    "        if parent_dir == current_dir:\n",
    "            raise FileNotFoundError(f\"Could not find {marker_filename} in any parent directory.\")\n",
    "        current_dir = parent_dir\n",
    "        \n",
    "# 경로/출력 폴더 생성(find_project_root() 호출 후 사용)\n",
    "def ensure_dir(path):\n",
    "    # 디렉토리 없으면 생성\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def get_project_path(*paths):\n",
    "    project_root = os.getcwd()    # or use a specific absolute path if needed\n",
    "    return os.path.join(project_root, *paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f921ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b64 encoder: jpg image -> base64 string / VLM API\n",
    "def encode_image_to_base64(\n",
    "    img_path,\n",
    "    jpeg_quality: int = 85,\n",
    "    max_size: int | None = 1024,   # None이면 리사이즈 안 함\n",
    "):\n",
    "    \"\"\"\n",
    "    - PNG 입력 시 → RGB JPEG로 변환 후 base64 인코딩\n",
    "    - JPEG 입력 시 → 그대로 (필요하면 리사이즈)\n",
    "    - max_size: 한 변 최대 길이 (planning/VLM 단계용)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img_path = Path(img_path)\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        # PNG / RGBA → RGB\n",
    "        if img.mode in (\"RGBA\", \"LA\", \"P\"):\n",
    "            img = img.convert(\"RGB\")\n",
    "\n",
    "        if max_size is not None:\n",
    "            img.thumbnail((max_size, max_size))\n",
    "\n",
    "        buf = io.BytesIO()\n",
    "\n",
    "        # PNG면 JPEG로 변환\n",
    "        if img_path.suffix.lower() == \".png\":\n",
    "            img.save(\n",
    "                buf,\n",
    "                format=\"JPEG\",\n",
    "                quality=jpeg_quality,\n",
    "                optimize=True,\n",
    "            )\n",
    "        else:\n",
    "            # jpg / jpeg 등\n",
    "            img.save(\n",
    "                buf,\n",
    "                format=\"JPEG\",\n",
    "                quality=jpeg_quality,\n",
    "                optimize=True,\n",
    "            )\n",
    "\n",
    "        buf.seek(0)\n",
    "        return base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding image: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d4f62b",
   "metadata": {},
   "source": [
    "1. VQA\n",
    "<br> input: error image + user image + user request + prompt_VQA\n",
    "<br> &nbsp;&nbsp;&nbsp;&nbsp;prompt_VQA:\n",
    "<br> LLM-VQA 호출 및 첫번째 응답 파싱\n",
    "<br> output: 요청과 이미지를 보고 판단한 내용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b4827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag parser\n",
    "\n",
    "import json\n",
    "import re\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "_TAG_RE = re.compile(\n",
    "    r\"<(?P<tag>analysis_log|plan_json)>(?P<body>[\\s\\S]*?)</(?P=tag)>\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "def parse_tagged_output(text: str) -> Tuple[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    LLM 응답에서 <analysis_log>와 <plan_json>을 추출하고,\n",
    "    plan_json은 JSON으로 파싱해서 dict로 반환.\n",
    "    \"\"\"\n",
    "    text = (text or \"\").strip()\n",
    "    matches = {m.group(\"tag\").lower(): m.group(\"body\").strip() for m in _TAG_RE.finditer(text)}\n",
    "\n",
    "    if \"analysis_log\" not in matches:\n",
    "        raise ValueError(\"응답에 <analysis_log>...</analysis_log> 태그가 없습니다.\")\n",
    "    if \"plan_json\" not in matches:\n",
    "        raise ValueError(\"응답에 <plan_json>...</plan_json> 태그가 없습니다.\")\n",
    "\n",
    "    analysis_log = matches[\"analysis_log\"]\n",
    "    plan_json_str = matches[\"plan_json\"]\n",
    "\n",
    "    try:\n",
    "        plan = json.loads(plan_json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"<plan_json> 내부 JSON 파싱 실패: {e}\\n\\nJSON:\\n{plan_json_str}\")\n",
    "\n",
    "    return analysis_log, plan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a24b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-VQA 호출 및 첫번째 응답 파싱\n",
    "\n",
    "from typing import Optional, Union\n",
    "from anthropic import Anthropic\n",
    "\n",
    "def model_response_anthropic(\n",
    "    anthropic_client: Anthropic,\n",
    "    prompt_text: str,\n",
    "    model: str = \"claude-3-7-sonnet-20250219\",\n",
    "    temperature: float = 0.1,\n",
    "    max_tokens: int = 1000,\n",
    "    parse_tags: bool = True,\n",
    "    print_log: bool = True,\n",
    "    # 이미지 입력(선택)\n",
    "    img_b64: Optional[str] = None,\n",
    "    media_type: str = \"image/png\",\n",
    ") -> Union[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Anthropic(Claude) 전용.\n",
    "    - prompt_text: prompt1 (태그 출력 규칙 포함)\n",
    "    - img_b64/media_type\n",
    "\n",
    "    Returns:\n",
    "      - parse_tags=False: raw string\n",
    "      - parse_tags=True: dict {\"raw\": str, \"analysis_log\": str, \"plan\": dict}\n",
    "    \"\"\"\n",
    "    # content 파트 구성\n",
    "    content = [{\"type\": \"text\", \"text\": prompt_text}]\n",
    "\n",
    "    if img_b64 is not None:\n",
    "        content.append({\n",
    "            \"type\": \"image\",\n",
    "            \"source\": {\n",
    "                \"type\": \"base64\",\n",
    "                \"media_type\": media_type,\n",
    "                \"data\": img_b64,\n",
    "            }\n",
    "        })\n",
    "\n",
    "    resp = anthropic_client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        messages=[{\"role\": \"user\", \"content\": content}],\n",
    "    )\n",
    "\n",
    "    # Claude 응답은 content blocks로 옴 → text block만 합치기\n",
    "    raw = \"\".join(\n",
    "        blk.text for blk in resp.content\n",
    "        if getattr(blk, \"type\", None) == \"text\"\n",
    "    ).strip()\n",
    "\n",
    "    if not parse_tags:\n",
    "        return raw\n",
    "\n",
    "    analysis_log, plan = parse_tagged_output(raw)\n",
    "\n",
    "    if print_log:\n",
    "        print(\"\\n[analysis_log]\\n\" + analysis_log)\n",
    "\n",
    "    return {\"raw\": raw, \"analysis_log\": analysis_log, \"plan\": plan}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0807f158",
   "metadata": {},
   "source": [
    "확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64df083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. VQA ===\n",
    "\n",
    "# 1-1) image, request\n",
    "    \n",
    "PROJECT_ROOT = Path(find_project_root())\n",
    "\n",
    "def get_project_path(*parts):\n",
    "    return PROJECT_ROOT.joinpath(*parts)\n",
    "\n",
    "env_path = get_project_path(\".env\")\n",
    "img = get_project_path(\"notebook\", \"data\", \"tomato_error.png\")\n",
    "user_request = \"semi-ripe 토마토 개수를 세어주세요.\"\n",
    "img_b64 = encode_image_to_base64(img, 80, 1024)\n",
    "\n",
    "display(Image.open(img).resize((512, 512)))\n",
    "print(\"인코딩 문자열의 앞 50자: \")\n",
    "print(img_b64[:50]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287bb06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-2) VQA propmt \n",
    "prompt_VQA = f\"\"\"\n",
    "You are an expert vision task planner.\n",
    "\n",
    "You will be given:\n",
    "- A user request (Korean)\n",
    "- ONE image (provided to you as an image input)\n",
    "\n",
    "Your job in this step is ONLY to analyze the user request and propose a concrete, tool-agnostic plan.\n",
    "Do NOT run code. Do NOT claim results. Do NOT hallucinate object counts.\n",
    "\n",
    "User request: {user_request}\n",
    "\n",
    "Output MUST contain EXACTLY TWO TAGS in this order:\n",
    "1) <analysis_log> ... </analysis_log>  (Korean, human-readable, step-by-step, short)\n",
    "2) <plan_json> ... </plan_json>        (machine-readable, MUST be valid JSON)\n",
    "\n",
    "Rules:\n",
    "- Do not output anything outside the two tags.\n",
    "- <analysis_log> should be concise: 5–10 lines, each starting with \"Step N:\".\n",
    "- <plan_json> must be STRICT JSON (no trailing commas, no comments, no markdown).\n",
    "\n",
    "<plan_json> JSON schema:\n",
    "{{\n",
    "  \"language\": \"ko\",\n",
    "  \"intent_summary\": string,\n",
    "  \"task_type\": \"counting\",\n",
    "  \"target_definition\": {{\n",
    "    \"primary_object\": \"tomato\",\n",
    "    \"required_attributes\": [\"red\"],\n",
    "    \"exclusions\": [string],\n",
    "    \"edge_cases\": [string]\n",
    "  }},\n",
    "  \"subtasks\": [\n",
    "    {{\n",
    "      \"name\": string,\n",
    "      \"goal\": string,\n",
    "      \"suggested_method\": string\n",
    "    }}\n",
    "  ],\n",
    "  \"tool_requirements\": {{\n",
    "    \"needs_localization\": boolean,\n",
    "    \"needs_instance_separation\": boolean,\n",
    "    \"needs_attribute_reasoning\": boolean,\n",
    "    \"preferred_outputs\": [string]\n",
    "  }},\n",
    "  \"verification_checks\": [string],\n",
    "  \"questions_if_ambiguous\": [string]\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a603a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-3) LLM 호출 및 응답 생성, 확인\n",
    "import os\n",
    "from anthropic import Anthropic\n",
    "\n",
    "anthropic_key  = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "if not anthropic_key:\n",
    "    raise RuntimeError(\"ANTHROPIC_API_KEY가 설정되어 있지 않습니다.\")\n",
    "\n",
    "\n",
    "anthropic_client = Anthropic(api_key=anthropic_key)\n",
    "\n",
    "out = model_response_anthropic(\n",
    "    anthropic_client=anthropic_client,\n",
    "    prompt_text=prompt_VQA,\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=1200,\n",
    "    temperature=0.2,\n",
    "    parse_tags=True,\n",
    "    print_log=True,\n",
    "    img_b64=img_b64,    \n",
    "    media_type=\"image/jpeg\", \n",
    ")\n",
    "\n",
    "plan = out[\"plan\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c165628b",
   "metadata": {},
   "source": [
    "2. Plan\n",
    "<br> input: error image + user request + LLM-VQA 응답 + prompt_plan\n",
    "<br> &nbsp;&nbsp;&nbsp;&nbsp; prompt_plan: tuned for smartfam environment(tool list 포함)\n",
    "<br> LLM-plan 호출 및 첫번째 응답 파싱\n",
    "<br> output: tool list에서 tool 선택 후의 계획\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c585476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, inspect\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "def load_visionagent_tools_strict() -> List[Dict[str, Any]]:\n",
    "    mod = importlib.import_module(\"vision_agent.tools.tools\")\n",
    "    tools: List[Dict[str, Any]] = []\n",
    "\n",
    "    for name, obj in inspect.getmembers(mod):\n",
    "        if not inspect.isfunction(obj) or name.startswith(\"_\"):\n",
    "            continue\n",
    "\n",
    "        # 1) tools.py에 \"정의된\" 함수만 (재export된 표준함수 제거)\n",
    "        if getattr(obj, \"__module__\", None) != mod.__name__:\n",
    "            continue\n",
    "\n",
    "        doc = inspect.getdoc(obj) or \"\"\n",
    "\n",
    "        # 2) '... is a tool' 패턴만 통과 (유틸 제거)\n",
    "        #    (원하면 조건 완화 가능)\n",
    "        if \" is a tool\" not in doc:\n",
    "            continue\n",
    "\n",
    "        tools.append({\n",
    "            \"name\": name,\n",
    "            \"qualname\": f\"{mod.__name__}.{name}\",\n",
    "            \"type\": \"function\",\n",
    "            \"signature\": str(inspect.signature(obj)),\n",
    "            \"doc\": doc,\n",
    "        })\n",
    "\n",
    "    return tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073a3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_tool_desc(\n",
    "    tools: List[Dict[str, Any]],\n",
    "    max_tools: int = 50,\n",
    "    max_doc_chars: int = 300,\n",
    ") -> str:\n",
    "    lines = []\n",
    "    for t in tools[:max_tools]:\n",
    "        lines.append(\n",
    "            f\"- {t['name']} ({t['type']})\\n\"\n",
    "            f\"  qualname: {t['qualname']}\\n\"\n",
    "            f\"  signature: {t['signature']}\\n\"\n",
    "            f\"  doc: {t['doc'][:max_doc_chars].replace('\\\\n', ' ')}\"\n",
    "        )\n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2501dc32",
   "metadata": {},
   "source": [
    "* 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ca5d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. Plan ===\n",
    "# 2-1) input\n",
    "\n",
    "vqa_log = out[\"analysis_log\"]\n",
    "vqa_struct = out[\"plan\"]\n",
    "\n",
    "# vqa_struct를 JSON 형태로 보기 좋게 출력\n",
    "print(\"=\" * 60)\n",
    "print(\"[VQA Plan Structure (JSON)]\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(vqa_struct, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1827e2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-2) tool list\n",
    "tools = load_visionagent_tools_strict()\n",
    "tool_desc = format_tool_desc(tools, max_tools=80, max_doc_chars=350)\n",
    "print(\"Loaded tools:\", len(tools))\n",
    "# print(tool_desc)\n",
    "print(\"qwen\" in tool_desc.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b66aa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-3) tool list 추가: detected_image_crop \n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480a80f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-4) prompt_plan\n",
    "prompt_PLAN = f\"\"\"\n",
    "You are a VisionAgent-style planner/controller.\n",
    "\n",
    "Your job is to decide the NEXT ACTION(s) to take using the available tools, based on the user's request and the accumulated evidence. You do NOT execute tools. You only output tool calls or the final answer.\n",
    "\n",
    "You will be given:\n",
    "- A user request (Korean)\n",
    "- ONE image (already annotated with detection boxes/labels overlaid)\n",
    "- Tool list with available actions\n",
    "- VQA log: chronological reasoning, detection notes, bounding-box/label evaluations, and any prior validation outcomes\n",
    "- VQA structured JSON summary of the detection/analysis results\n",
    "- Prior tool observations may also appear in the conversation history (as \"observation\").\n",
    "\n",
    "Primary evidence:\n",
    "- Build decisions primarily from [VQA_LOG] and [VQA_STRUCT_JSON].\n",
    "- Do NOT hallucinate new detections, boxes, or attributes beyond the provided evidence and tool outputs.\n",
    "\n",
    "User request (Korean):\n",
    "{user_request}\n",
    "\n",
    "[VQA_LOG]\n",
    "{vqa_log}\n",
    "\n",
    "[VQA_STRUCT_JSON]\n",
    "{json.dumps(vqa_struct, ensure_ascii=False, indent=2)}\n",
    "\n",
    "You MUST use ONLY the tools listed in [TOOLS] below.\n",
    "Do NOT invent, rename, or assume any tools beyond this list.\n",
    "\n",
    "[TOOLS]\n",
    "{tool_desc}\n",
    "\n",
    "────────────────────────────────\n",
    "CORE CONTROL LOOP BEHAVIOR\n",
    "────────────────────────────────\n",
    "At each turn, output either:\n",
    "(A) Tool calls for the NEXT immediate actions (one or more tool calls), OR\n",
    "(B) A final answer if no more tools are needed.\n",
    "\n",
    "Do NOT output a full end-to-end plan. Do NOT output steps[1..N].\n",
    "The executor will run your tool calls in the order you provide, append observations, and call you again with updated context.\n",
    "\n",
    "────────────────────────────────\n",
    "DETECTION-SPECIFIC BEHAVIOR\n",
    "────────────────────────────────\n",
    "- The image is already annotated. Prefer verification of existing detections.\n",
    "- If bounding-box coordinates are available in VQA_STRUCT_JSON, use them for cropping and verification.\n",
    "- If bounding-box coordinates are NOT available, do NOT guess. Request the annotation file\n",
    "  (COCO JSON / YOLO TXT / model output JSON) or propose a concrete method to obtain coordinates.\n",
    "\n",
    "Recommended strategy for detection verification tasks:\n",
    "1) Crop each existing detected region (one-by-one or batch) and verify whether it matches its label.\n",
    "2) If misdetections or missing objects are suspected, run re-detection (only if a tool exists) and verify again.\n",
    "3) When confident, produce the final count/summary.\n",
    "\n",
    "────────────────────────────────\n",
    "OUTPUT FORMAT (STRICT)\n",
    "────────────────────────────────\n",
    "Output MUST contain EXACTLY TWO tags in this exact order:\n",
    "1) <analysis_log> ... </analysis_log>\n",
    "2) <action_json> ... </action_json>\n",
    "\n",
    "Do NOT output anything outside the two tags.\n",
    "\n",
    "<analysis_log> rules:\n",
    "- 3–7 lines only\n",
    "- Each line must start with \"Step N:\"\n",
    "- Only describe the immediate reasoning for the NEXT action(s), not a full multi-step plan.\n",
    "\n",
    "<action_json> rules:\n",
    "- MUST be STRICT JSON (no trailing commas, no comments, no markdown)\n",
    "- Must match exactly one of the following schemas:\n",
    "\n",
    "Schema 1: Tool calls\n",
    "{\n",
    "  \"language\": \"ko\",\n",
    "  \"mode\": \"tool_calls\",\n",
    "  \"selected_tools\": [string],\n",
    "  \"tool_calls\": [\n",
    "    {\n",
    "      \"id\": int,\n",
    "      \"tool\": string,\n",
    "      \"parameters\": object,\n",
    "      \"expected_result\": string\n",
    "    }\n",
    "  ],\n",
    "  \"open_questions\": [string]\n",
    "}\n",
    "\n",
    "Schema 2: Final answer\n",
    "{\n",
    "  \"language\": \"ko\",\n",
    "  \"mode\": \"final\",\n",
    "  \"final_answer\": string,\n",
    "  \"open_questions\": [string]\n",
    "}\n",
    "\n",
    "Additional rules:\n",
    "- tool_calls must be listed in exact execution order; ids must start at 1 and increase strictly by 1 within this turn.\n",
    "- Each tool call MUST reference a tool name from [TOOLS].\n",
    "- Keep tool_calls minimal: only what is needed before the next observation.\n",
    "- If you need missing inputs (e.g., box coordinates), set mode=\"final\" and clearly request them in final_answer, or set open_questions accordingly.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9297e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from winsound import PlaySound\n",
    "\n",
    "out_plan = model_response_anthropic(\n",
    "    anthropic_client=anthropic_client,\n",
    "    prompt_text=prompt_PLAN,\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=1400,\n",
    "    temperature=0.2,\n",
    "    parse_tags=True,\n",
    "    print_log=True,\n",
    "    img_b64=img_b64,\n",
    "    media_type=\"image/jpeg\",\n",
    ")\n",
    "\n",
    "plan_struct = out_plan[\"plan\"]\n",
    "\n",
    "PlaySound_struct = out[\"plan\"]\n",
    "\n",
    "# vqa_struct를 JSON 형태로 보기 좋게 출력\n",
    "print(\"=\" * 60)\n",
    "print(\"[VQA Plan Structure (JSON)]\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(plan_struct, indent=2, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
