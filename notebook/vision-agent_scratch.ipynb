{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d79166c",
   "metadata": {},
   "source": [
    "### Vision Detection Agent scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b116f0a",
   "metadata": {},
   "source": [
    "⸻"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b1baa7",
   "metadata": {},
   "source": [
    "* 전체 과정 \n",
    "<br> Input → Preparing → VQA → [ Plannning +  Executing ] → Code Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d401c3",
   "metadata": {},
   "source": [
    "* 개선점\n",
    "1) 단계별 계획: 이전 계획과 tool을 참고해 계획 생성 \n",
    "2) 사전의 tool을 제공\n",
    "3) LLM 답변의 일관성 확보를 위한 프롬프트 규약 강화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a115d71",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515b0280",
   "metadata": {},
   "source": [
    "* Input\n",
    "    * image\n",
    "    * request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3963dc74",
   "metadata": {},
   "source": [
    "* AgentState 데이터구조 \n",
    "    * state1 → state2 → state3 →  ... \n",
    "    * 생성된 내용을 저장해 - 디버깅 + 재현성 확보 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f387adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "@dataclass\n",
    "class AgentState:\n",
    "    # 고정 입력/환경\n",
    "    user_request: str\n",
    "    img_b64: Optional[str] = None\n",
    "    tool_desc: str = \"\"\n",
    "    tool_registry: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    # 누적 상태(증거)\n",
    "    vqa_log: str = \"\"\n",
    "    vqa_struct: Dict[str, Any] = field(default_factory=dict)\n",
    "    observations: List[Dict[str, Any]] = field(default_factory=list)\n",
    "\n",
    "    # 기록(리플레이/디버깅)\n",
    "    all_plans: List[Dict[str, Any]] = field(default_factory=list)\n",
    "    all_execs: List[Dict[str, Any]] = field(default_factory=list)\n",
    "\n",
    "    # 종료\n",
    "    final_answer: Optional[str] = None\n",
    "\n",
    "    # final plan 결과\n",
    "    code_plan: Optional[List[Dict[str, Any]]] = None\n",
    "\n",
    "    # coder \n",
    "    coder: Optional[Any] = None  # 타입 고정하려면 Optional[CodeCoder]로\n",
    "    coder_prompt: Optional[str] = None\n",
    "    generated_code: Optional[str] = None\n",
    "    generated_code_path: Optional[str] = None\n",
    "    all_codes: List[Dict[str, Any]] = field(default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748aeaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import io\n",
    "import base64\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from anthropic import Anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04f9573",
   "metadata": {},
   "source": [
    "* Preparing\n",
    "1. image-> base64: LLM이 이미지를 이해하는 형태의 데이터로 변환\n",
    "2. tool list: vision agent에서 제공하는 tool의 meta data를 LLM의 입력 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff87dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image to Base64\n",
    "def encode_image_to_base64(\n",
    "    img_path,\n",
    "    jpeg_quality: int = 85,\n",
    "    max_size: int | None = 1024,   # None이면 리사이즈 안 함\n",
    "):\n",
    "    \"\"\"\n",
    "    - PNG 입력 시 → RGB JPEG로 변환 후 base64 인코딩\n",
    "    - JPEG 입력 시 → 그대로 (필요하면 리사이즈)\n",
    "    - max_size: 한 변 최대 길이 (planning/VLM 단계용)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img_path = Path(img_path)\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        # PNG / RGBA → RGB\n",
    "        if img.mode in (\"RGBA\", \"LA\", \"P\"):\n",
    "            img = img.convert(\"RGB\")\n",
    "\n",
    "        if max_size is not None:\n",
    "            img.thumbnail((max_size, max_size)) # 크기 제한\n",
    "\n",
    "        buf = io.BytesIO()\n",
    "\n",
    "        # PNG면 JPEG로 변환\n",
    "        if img_path.suffix.lower() == \".png\":\n",
    "            img.save(\n",
    "                buf,\n",
    "                format=\"JPEG\",\n",
    "                quality=jpeg_quality,\n",
    "                optimize=True,\n",
    "            )\n",
    "        else:\n",
    "            # jpg / jpeg 등\n",
    "            img.save(\n",
    "                buf,\n",
    "                format=\"JPEG\",\n",
    "                quality=jpeg_quality,\n",
    "                optimize=True,\n",
    "            )\n",
    "\n",
    "        buf.seek(0)\n",
    "        return base64.b64encode(buf.read()).decode(\"utf-8\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding image: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7891ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool registry\n",
    "tool_registry = {}\n",
    "for tool in tools:\n",
    "    tool_registry[tool[\"name\"]] = {\n",
    "        \"func\": 실제_함수_참조,\n",
    "        \"metadata\": tool  # name, doc, signature 등\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983901d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_context(\n",
    "    user_request: str,\n",
    "    img_path: Optional[str],\n",
    "    tools: List[Dict[str, Any]],\n",
    ") -> AgentState:\n",
    "    import importlib\n",
    "    import vision_agent.tools.tools as tools_mod\n",
    "    \n",
    "    img_b64 = encode_image_to_base64(img_path) if img_path else None\n",
    "\n",
    "    # tool_registry를 만들 때 실제 함수도 포함\n",
    "    tool_registry = {}\n",
    "    for t in tools:\n",
    "        tool_name = t[\"name\"]\n",
    "        # metadata dict 복사\n",
    "        tool_registry[tool_name] = t.copy()\n",
    "        \n",
    "        # 실제 함수 가져오기\n",
    "        if hasattr(tools_mod, tool_name):\n",
    "            func = getattr(tools_mod, tool_name)\n",
    "            if callable(func):\n",
    "                tool_registry[tool_name][\"func\"] = func\n",
    "    \n",
    "    tool_desc = format_tool_desc(tools, max_tools=80, max_doc_chars=350)\n",
    "\n",
    "    assert len(tool_registry) > 0\n",
    "    return AgentState(\n",
    "        user_request=user_request,\n",
    "        img_b64=img_b64,\n",
    "        tool_desc=tool_desc,\n",
    "        tool_registry=tool_registry,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87c62d6",
   "metadata": {},
   "source": [
    "* VQA\n",
    "    * multi modality 입력 구성 \n",
    "    * PROMPT_VQA\n",
    "    <br> <analysis_log> ... <analysis_log>  / <plan_json> ... <plan_json>\n",
    "    <br> 형태의 구조화된 출력을 프롬프트에 명시 - LLM의 출력을 강제\n",
    "    * LLM 응답 파싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4722f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag parser\n",
    "\n",
    "import json\n",
    "import re\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "_TAG_RE = re.compile(\n",
    "    r\"<(?P<tag>analysis_log|plan_json)>(?P<body>[\\s\\S]*?)</(?P=tag)>\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "def parse_tagged_output(text: str) -> Tuple[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    LLM 응답에서 <analysis_log>와 <plan_json>을 추출하고,\n",
    "    plan_json은 JSON으로 파싱해서 dict로 반환\n",
    "    \"\"\"\n",
    "    text = (text or \"\").strip()\n",
    "    matches = {m.group(\"tag\").lower(): m.group(\"body\").strip() for m in _TAG_RE.finditer(text)}\n",
    "\n",
    "    if \"analysis_log\" not in matches:\n",
    "        raise ValueError(\"응답에 <analysis_log>...</analysis_log> 태그가 없습니다.\")\n",
    "    if \"plan_json\" not in matches:\n",
    "        raise ValueError(\"응답에 <plan_json>...</plan_json> 태그가 없습니다.\")\n",
    "\n",
    "    analysis_log = matches[\"analysis_log\"]\n",
    "    plan_json_str = matches[\"plan_json\"]\n",
    "\n",
    "    try:\n",
    "        plan = json.loads(plan_json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"<plan_json> 내부 JSON 파싱 실패: {e}\\n\\nJSON:\\n{plan_json_str}\")\n",
    "\n",
    "    return analysis_log, plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b760a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_VQA_TEMPLATE = \"\"\"\n",
    "You are an expert vision task planner.\n",
    "\n",
    "You will be given:\n",
    "- A user request (Korean)\n",
    "- ONE image (provided to you as an image input)\n",
    "\n",
    "Your job in this step is ONLY to analyze the user request and propose a concrete, tool-agnostic plan.\n",
    "Do NOT run code. Do NOT claim results. Do NOT hallucinate object counts.\n",
    "\n",
    "User request: {user_request}\n",
    "\n",
    "Output MUST contain EXACTLY TWO TAGS in this order:\n",
    "1) <analysis_log> ... </analysis_log>  (Korean, human-readable, step-by-step, short)\n",
    "2) <plan_json> ... </plan_json>        (machine-readable, MUST be valid JSON)\n",
    "\n",
    "Rules:\n",
    "- Do not output anything outside the two tags.\n",
    "- <analysis_log> should be concise: 5–10 lines, each starting with \"Step N:\".\n",
    "- <plan_json> must be STRICT JSON (no trailing commas, no comments, no markdown).\n",
    "\n",
    "<plan_json> JSON schema:\n",
    "{{\n",
    "  \"language\": \"ko\",\n",
    "  \"intent_summary\": string,\n",
    "  \"task_type\": \"counting\",\n",
    "  \"target_definition\": {{\n",
    "    \"primary_object\": \"tomato\",\n",
    "    \"required_attributes\": [\"red\"],\n",
    "    \"exclusions\": [string],\n",
    "    \"edge_cases\": [string]\n",
    "  }},\n",
    "  \"subtasks\": [\n",
    "    {{\n",
    "      \"name\": string,\n",
    "      \"goal\": string,\n",
    "      \"suggested_method\": string\n",
    "    }}\n",
    "  ],\n",
    "  \"tool_requirements\": {{\n",
    "    \"needs_localization\": boolean,\n",
    "    \"needs_instance_separation\": boolean,\n",
    "    \"needs_attribute_reasoning\": boolean,\n",
    "    \"preferred_outputs\": [string]\n",
    "  }},\n",
    "  \"verification_checks\": [string],\n",
    "  \"questions_if_ambiguous\": [string]\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ac08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "from anthropic import Anthropic\n",
    "\n",
    "def model_response_anthropic(\n",
    "    anthropic_client: Anthropic,\n",
    "    prompt_text: str,\n",
    "    model: str = \"claude-sonnet-4-20250514\",\n",
    "    temperature: float = 0.1,\n",
    "    max_tokens: int = 1000,\n",
    "    parse_tags: bool = True,\n",
    "    print_log: bool = True,\n",
    "    img_b64: Optional[str] = None,\n",
    "    media_type: str = \"image/png\",\n",
    ") -> Union[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Anthropic(Claude) 전용.\n",
    "    - prompt_text: prompt1 (태그 출력 규칙 포함)\n",
    "    - img_b64/media_type\n",
    "\n",
    "    Returns:\n",
    "      - parse_tags=False: raw string\n",
    "      - parse_tags=True: dict {\"raw\": str, \"analysis_log\": str, \"plan\": dict}\n",
    "    \"\"\"\n",
    "    # multi modality 입력 구성\n",
    "    content = [{\"type\": \"text\", \"text\": prompt_text}]\n",
    "\n",
    "    if img_b64 is not None:\n",
    "        content.append({\n",
    "            \"type\": \"image\",\n",
    "            \"source\": {\n",
    "                \"type\": \"base64\",\n",
    "                \"media_type\": media_type,\n",
    "                \"data\": img_b64,\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # LLM 응답 생성\n",
    "    resp = anthropic_client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        messages=[{\"role\": \"user\", \"content\": content}],\n",
    "    )\n",
    "\n",
    "    # Claude 응답은 content blocks로 옴 → text block만 합치기\n",
    "    raw = \"\".join(\n",
    "        blk.text for blk in resp.content\n",
    "        if getattr(blk, \"type\", None) == \"text\"\n",
    "    ).strip()\n",
    "\n",
    "    if not parse_tags:\n",
    "        return raw\n",
    "\n",
    "    # 태그 파싱\n",
    "    analysis_log, plan = parse_tagged_output(raw)\n",
    "\n",
    "    if print_log:\n",
    "        print(\"\\n[analysis_log]\\n\" + analysis_log)\n",
    "\n",
    "    return {\"raw\": raw, \"analysis_log\": analysis_log, \"plan\": plan}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824115af",
   "metadata": {},
   "source": [
    "* Planning\n",
    "    * 도구에 대한 설명을 구조화 \n",
    "    * PROMPT_PLAN\n",
    "    * 각 단계별 observation을 PROMPT_PLAN에 추가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568b38ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_tool_desc(\n",
    "    tools: List[Dict[str, Any]],\n",
    "    max_tools: int = 50,\n",
    "    max_doc_chars: int = 300,\n",
    ") -> str:\n",
    "    lines = []\n",
    "    for t in tools[:max_tools]:\n",
    "        lines.append(\n",
    "            f\"- {t['name']} ({t['type']})\\n\"\n",
    "            f\"  qualname: {t['qualname']}\\n\"\n",
    "            f\"  signature: {t['signature']}\\n\"\n",
    "            f\"  doc: {t['doc'][:max_doc_chars].replace('\\\\n', ' ')}\"\n",
    "        )\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba1d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_PLAN_TEMPLATE = \"\"\"\n",
    "You are a VisionAgent-style planner/controller.\n",
    "\n",
    "Your job is to decide the NEXT ACTION(s) to take using the available tools, based on the user's request and the accumulated evidence. You do NOT execute tools. You only output tool calls or the final answer.\n",
    "\n",
    "You will be given:\n",
    "- A user request (Korean)\n",
    "- ONE image (already annotated with detection boxes/labels overlaid)\n",
    "- Tool list with available actions\n",
    "- VQA log: chronological reasoning, detection notes, bounding-box/label evaluations, and any prior validation outcomes\n",
    "- VQA structured JSON summary of the detection/analysis results\n",
    "- Prior tool observations may also appear in the conversation history (as \"observation\").\n",
    "\n",
    "Primary evidence:\n",
    "- Build decisions primarily from [VQA_LOG] and [VQA_STRUCT_JSON].\n",
    "- Do NOT hallucinate new detections, boxes, or attributes beyond the provided evidence and tool outputs.\n",
    "\n",
    "User request (Korean):\n",
    "{user_request}\n",
    "\n",
    "[VQA_LOG]\n",
    "{vqa_log}\n",
    "\n",
    "[VQA_STRUCT_JSON]\n",
    "{vqa_struct_json}\n",
    "\n",
    "[TOOLS]\n",
    "{tool_desc}\n",
    "\n",
    "[OBSERVATIONS]\n",
    "{observations}\n",
    "\n",
    "────────────────────────────────\n",
    "CORE CONTROL LOOP BEHAVIOR\n",
    "────────────────────────────────\n",
    "At each turn, output either:\n",
    "(A) Tool calls for the NEXT immediate actions (one or more tool calls), OR\n",
    "(B) A final answer if no more tools are needed.\n",
    "\n",
    "Do NOT output a full end-to-end plan. Do NOT output steps[1..N].\n",
    "The executor will run your tool calls in the order you provide, append observations, and call you again with updated context.\n",
    "\n",
    "────────────────────────────────\n",
    "DETECTION-SPECIFIC BEHAVIOR\n",
    "────────────────────────────────\n",
    "- The image is already annotated. Prefer verification of existing detections.\n",
    "- If bounding-box coordinates are available in VQA_STRUCT_JSON, use them for cropping and verification.\n",
    "- If bounding-box coordinates are NOT available, do NOT guess. Request the annotation file\n",
    "  (COCO JSON / YOLO TXT / model output JSON) or propose a concrete method to obtain coordinates.\n",
    "\n",
    "Hard rule:\n",
    "- If VQA_STRUCT_JSON contains bbox coordinates, you MUST call the crop tool first for those boxes.\n",
    "- You MUST NOT call any VQA tool before attempting crop-based verification when bbox coordinates exist.\n",
    "- VQA tools are allowed ONLY if bbox coordinates are missing/unavailable OR cropping fails with an error.\n",
    "\n",
    "────────────────────────────────\n",
    "OUTPUT FORMAT (STRICT)\n",
    "────────────────────────────────\n",
    "Output MUST contain EXACTLY TWO tags in this exact order:\n",
    "1) <analysis_log> ... </analysis_log>\n",
    "2) <plan_json> ... </plan_json>\n",
    "\n",
    "Do NOT output anything outside the two tags.\n",
    "\n",
    "<analysis_log> rules:\n",
    "- 3–7 lines only\n",
    "- Each line must start with \"Step N:\"\n",
    "- Only describe the immediate reasoning for the NEXT action(s), not a full multi-step plan.\n",
    "\n",
    "<plan_json> rules:\n",
    "- MUST be STRICT JSON (no trailing commas, no comments, no markdown)\n",
    "- Must match exactly one of the following schemas:\n",
    "\n",
    "Schema 1: Tool calls\n",
    "{{\n",
    "  \"language\": \"ko\",\n",
    "  \"mode\": \"tool_calls\",\n",
    "  \"selected_tools\": [string],\n",
    "  \"tool_calls\": [\n",
    "    {{\n",
    "      \"id\": int,\n",
    "      \"tool\": string,\n",
    "      \"parameters\": object,\n",
    "      \"expected_result\": string\n",
    "    }}\n",
    "  ],\n",
    "  \"open_questions\": [string]\n",
    "}}\n",
    "\n",
    "Schema 2: Final answer\n",
    "{{\n",
    "  \"language\": \"ko\",\n",
    "  \"mode\": \"final\",\n",
    "  \"final_answer\": string,\n",
    "  \"open_questions\": [string]\n",
    "}}\n",
    "\n",
    "Additional rules:\n",
    "- tool_calls must be listed in exact execution order; ids must start at 1 and increase strictly by 1 within this turn.\n",
    "- Each tool call MUST reference a tool name from [TOOLS].\n",
    "- Keep tool_calls minimal: only what is needed before the next observation.\n",
    "- If you need missing inputs (e.g., box coordinates), set mode=\"final\" and clearly request them in final_answer, or set open_questions accordingly.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c305262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_prompt(user_request: str, vqa_log: str, vqa_struct: dict, tool_desc: str) -> str:\n",
    "    vqa_struct_json = json.dumps(vqa_struct, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # observations는 너무 길어질 수 있으니, 필요하면 truncate 가능\n",
    "    if planner_state[\"observations\"]:\n",
    "        obs_text = json.dumps(planner_state[\"observations\"], ensure_ascii=False, indent=2)\n",
    "    else:\n",
    "        obs_text = \"(none)\"\n",
    "\n",
    "    return PROMPT_PLAN_TEMPLATE.format(\n",
    "        user_request=user_request,\n",
    "        vqa_log=vqa_log,\n",
    "        vqa_struct_json=vqa_struct_json,\n",
    "        tool_desc=tool_desc,\n",
    "        observations=obs_text,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b77bb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plan mode checking\n",
    "def validate_plan(plan: dict, tools_meta: list[dict]) -> None:\n",
    "    names = {t[\"name\"] for t in tools_meta}\n",
    "    mode = plan.get(\"mode\")\n",
    "    if mode not in (\"tool_calls\", \"final\"):\n",
    "        raise ValueError(f\"Invalid mode: {mode}\")\n",
    "\n",
    "    if mode == \"tool_calls\":\n",
    "        for tc in plan.get(\"tool_calls\", []):\n",
    "            if tc.get(\"tool\") not in names:\n",
    "                raise ValueError(f\"Unknown tool in plan: {tc.get('tool')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9a7647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_observation(tool: str, params: dict, result, ok: bool = True, error: str | None = None):\n",
    "    planner_state[\"observations\"].append({\n",
    "        \"tool\": tool,\n",
    "        \"parameters\": params,\n",
    "        \"ok\": ok,\n",
    "        \"result\": result,\n",
    "        \"error\": error,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ad2453",
   "metadata": {},
   "source": [
    "* Executing\n",
    "    * plan에서 선택한 도구 실행\n",
    "    * base64 형태의 이미지 데이터는 도구가 이미지를 다룰 수 있도록 numpy 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e308f5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tool_call(tool_call: dict, tool_registry: dict):\n",
    "    \"\"\"\n",
    "    Execute a single tool call safely.\n",
    "\n",
    "    Input:\n",
    "      - tool_call: {\"tool\": str, \"parameters\": dict, ...}\n",
    "      - tool_registry: {tool_name: callable}\n",
    "\n",
    "    Output:\n",
    "      {\n",
    "        \"tool\": str,\n",
    "        \"ok\": bool,\n",
    "        \"result\": Any | None,\n",
    "        \"error\": str | None,\n",
    "      }\n",
    "    \"\"\"\n",
    "    tool_name = tool_call.get(\"tool\")\n",
    "    params = tool_call.get(\"parameters\", {})\n",
    "\n",
    "    # 1) tool 존재 여부 확인\n",
    "    if tool_name not in tool_registry:\n",
    "        return {\n",
    "            \"tool\": tool_name,\n",
    "            \"ok\": False,\n",
    "            \"result\": None,\n",
    "            \"error\": f\"Unknown tool: {tool_name}\",\n",
    "        }\n",
    "\n",
    "    fn = tool_registry[tool_name]\n",
    "\n",
    "    # 2) 실행 + 예외 처리\n",
    "    try:\n",
    "        result = fn(**params)\n",
    "        return {\n",
    "            \"tool\": tool_name,\n",
    "            \"ok\": True,\n",
    "            \"result\": result,\n",
    "            \"error\": None,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"tool\": tool_name,\n",
    "            \"ok\": False,\n",
    "            \"result\": None,\n",
    "            \"error\": repr(e),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57994554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b64_to_np(img_b64: str) -> np.ndarray:\n",
    "    data = base64.b64decode(img_b64)\n",
    "    img = Image.open(io.BytesIO(data)).convert(\"RGB\")\n",
    "    arr = np.array(img)\n",
    "    assert isinstance(arr, np.ndarray)\n",
    "    assert arr.ndim == 3 and arr.shape[2] == 3\n",
    "    return arr\n",
    "\n",
    "IMG_NP = b64_to_np(img_b64)  # 한번만 만들어서 재사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e029d6",
   "metadata": {},
   "source": [
    "* Agentic Loop\n",
    "    * 반복적 개선 매커니즘\n",
    "    * 이전 observation을 참고해 계획 수정\n",
    "    * 상태 누적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad35e6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for turn in range(1, max_turns + 1):\n",
    "    plan = plan_next(state, client, PROMPT_PLAN)\n",
    "    \n",
    "    if plan.mode == \"tool_calls\":\n",
    "        state = execute_plan(state, plan)  # observations 누적\n",
    "        \n",
    "    if plan.mode == \"final\":\n",
    "        final_plan = generate_final_plan(state, client, PROMPT_FINAL_PLAN)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d6b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.observations.append(exec_result)\n",
    "state.all_plans.append(plan_json)\n",
    "state.all_execs.append(exec_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91db95c",
   "metadata": {},
   "source": [
    "* Code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b6eba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_FINAL_PLAN_TEMPLATE = \"\"\"\n",
    "You are a code planning expert. Your job is to create a detailed, step-by-step execution plan for generating Python code.\n",
    "\n",
    "You will be given:\n",
    "- A user request (Korean)\n",
    "- VQA analysis results (what was understood from the image and request)\n",
    "- Tool execution observations (what tools were run and their results)\n",
    "- Available tools list\n",
    "\n",
    "Your task:\n",
    "Create a final execution plan that lists the exact steps needed to write Python code to complete the task.\n",
    "\n",
    "User request: {user_request}\n",
    "\n",
    "[VQA ANALYSIS]\n",
    "{vqa_log}\n",
    "\n",
    "[VQA STRUCTURED SUMMARY]\n",
    "{vqa_struct_json}\n",
    "\n",
    "[TOOL OBSERVATIONS]\n",
    "{observations}\n",
    "\n",
    "[AVAILABLE TOOLS]\n",
    "{tool_desc}\n",
    "\n",
    "────────────────────────────────\n",
    "OUTPUT FORMAT (STRICT)\n",
    "────────────────────────────────\n",
    "Output MUST contain EXACTLY TWO tags in this exact order:\n",
    "1) <final_answer> ... </final_answer>\n",
    "2) <code_plan> ... </code_plan>\n",
    "\n",
    "<final_answer> rules:\n",
    "- Brief summary (1-2 sentences) of what the code will accomplish\n",
    "- Written in Korean\n",
    "\n",
    "<code_plan> rules:\n",
    "- MUST be STRICT JSON array\n",
    "- Each element represents one execution step\n",
    "- Steps must be in exact execution order\n",
    "- Format:\n",
    "[\n",
    "  {{\n",
    "    \"step\": 1,\n",
    "    \"instruction\": \"Brief instruction describing what to do\",\n",
    "    \"code_snippet\": \"Example code (not full implementation, just example)\",\n",
    "    \"explanation\": \"Why this step is needed (optional)\"\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "Instruction guidelines:\n",
    "- Be specific about which functions/tools to use\n",
    "- Include parameter hints (e.g., \"prompt 'tomato'\")\n",
    "- Each step should be a single, clear action\n",
    "- Steps should build on each other logically\n",
    "\n",
    "Example instruction format:\n",
    "- \"Load the image using load_image()\"\n",
    "- \"Use countgd_object_detection with prompt 'tomato' to detect all tomato instances\"\n",
    "- \"Count the number of detections by getting the length of the detection list\"\n",
    "- \"Visualize the detections by overlaying bounding boxes using overlay_bounding_boxes()\"\n",
    "- \"Save the visualization to a file using save_image()\"\n",
    "\n",
    "Do NOT output anything outside the two tags.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07dbf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_plan(\n",
    "    state: AgentState,\n",
    "    anthropic_client: Anthropic,\n",
    "    prompt_template: str = PROMPT_FINAL_PLAN_TEMPLATE,\n",
    "    model: str = \"claude-sonnet-4-20250514\",\n",
    "    temperature: float = 0.2,\n",
    "    max_tokens: int = 1500,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    최종 코드 생성 계획을 생성\n",
    "    \n",
    "    Returns:\n",
    "        {\n",
    "            \"final_answer\": str,\n",
    "            \"code_plan\": List[Dict[str, Any]]\n",
    "        }\n",
    "    \"\"\"\n",
    "    prompt = prompt_template.format(\n",
    "        user_request=state.user_request,\n",
    "        vqa_log=state.vqa_log,\n",
    "        vqa_struct_json=json.dumps(state.vqa_struct, ensure_ascii=False, indent=2),\n",
    "        observations=json.dumps(state.observations, ensure_ascii=False, indent=2) if state.observations else \"(none)\",\n",
    "        tool_desc=state.tool_desc,\n",
    "    )\n",
    "    \n",
    "    out = model_response_anthropic(\n",
    "        anthropic_client=anthropic_client,\n",
    "        prompt_text=prompt,\n",
    "        model=model,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        parse_tags=True,\n",
    "        print_log=False,\n",
    "        img_b64=state.img_b64,\n",
    "        media_type=\"image/jpeg\",\n",
    "    )\n",
    "    \n",
    "    # 태그 파싱\n",
    "    raw = out[\"raw\"]\n",
    "    \n",
    "    # <final_answer> 태그 추출\n",
    "    final_answer_match = re.search(r\"<final_answer>(.*?)</final_answer>\", raw, re.DOTALL | re.IGNORECASE)\n",
    "    final_answer = final_answer_match.group(1).strip() if final_answer_match else \"\"\n",
    "    \n",
    "    # <code_plan> 태그 추출 및 JSON 파싱\n",
    "    code_plan_match = re.search(r\"<code_plan>(.*?)</code_plan>\", raw, re.DOTALL | re.IGNORECASE)\n",
    "    if not code_plan_match:\n",
    "        raise ValueError(\"응답에 <code_plan>...</code_plan> 태그가 없습니다.\")\n",
    "    \n",
    "    code_plan_str = code_plan_match.group(1).strip()\n",
    "    try:\n",
    "        code_plan = json.loads(code_plan_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"<code_plan> 내부 JSON 파싱 실패: {e}\\n\\nJSON:\\n{code_plan_str}\")\n",
    "    \n",
    "    return {\n",
    "        \"final_answer\": final_answer,\n",
    "        \"code_plan\": code_plan,\n",
    "        \"raw\": raw\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7366e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_codegen_prompt(instruction: str, tool_desc: str = \"\", has_image: bool = False) -> str:\n",
    "    img_note = (\n",
    "        \"- An image is provided via base64 (img_b64). Use it ONLY if your environment supports it.\\n\"\n",
    "        if has_image else\n",
    "        \"- No image is directly embedded. Assume the script will load image_path from disk.\\n\"\n",
    "    )\n",
    "\n",
    "    return f\"\"\"\n",
    "You are a coding assistant.\n",
    "Write a SINGLE Python file that satisfies the instruction below.\n",
    "\n",
    "Instruction:\n",
    "{instruction}\n",
    "\n",
    "Available tools (reference only):\n",
    "{tool_desc}\n",
    "\n",
    "Hard requirements:\n",
    "- Output ONLY valid Python code (no markdown, no explanations).\n",
    "- The file must be executable as a script.\n",
    "- Provide: run(image_path: str) -> dict\n",
    "- Include a __main__ block that calls run(\"image.png\") by default.\n",
    "{img_note}\n",
    "- Include needed imports explicitly.\n",
    "- Make it robust: basic error handling and clear variable names.\n",
    "- Do NOT print the whole image or base64. Only print summary results.\n",
    "\n",
    "Return only the code.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def strip_code_fences(text: str) -> str:\n",
    "    t = text.strip()\n",
    "    if t.startswith(\"```\"):\n",
    "        t = t.split(\"\\n\", 1)[-1]\n",
    "        if t.endswith(\"```\"):\n",
    "            t = t.rsplit(\"```\", 1)[0]\n",
    "    return t.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6c7c52",
   "metadata": {},
   "source": [
    "* 한계점\n",
    "1. 타 LLM을 사용한 validation 과정의 추가 필요\n",
    "2. LLM judge & reasoning 과정에 scorind 필요 \n",
    "3. vision agent에서의 도구 외의 새로운 도구를 추가 필요 - 방법 고안"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
