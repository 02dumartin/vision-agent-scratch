{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d79166c",
   "metadata": {},
   "source": [
    "### Vision Detection Agent scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b116f0a",
   "metadata": {},
   "source": [
    "⸻"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0206ef7d",
   "metadata": {},
   "source": [
    "### 10 step\n",
    "\n",
    "* [1-3 step]: 이해 및 계획\n",
    "<br> 사용자 요청 분석 → 이미지 로드: VQA(Visual Question Answering)로 이미지 이해\n",
    "\n",
    "* [4-6 step]: 도구 선택\n",
    "<br> suggestion() → get_tool_for_task() → 최적 도구 결정\n",
    "\n",
    "* [7-9 step]: 실행 및 검증\n",
    "\n",
    "* [10 step]: 최종 코드 생성 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c6bd05",
   "metadata": {},
   "source": [
    "#### [1-3 step]: 이해 및 계획\n",
    "ImageLoader - 이미지 파일 로드 / 이미지 리사이즈 / PIL to Numpy / 유효성\n",
    "<br> VQAModel - 이미지에 대한 질문 / 전체 설명 / 특정 객체 개수 추정\n",
    "<br> Planner - LMM 초기화/ 사용자 요청 분석 / 단계별 계획 생성/ 계획 to markdown / 검증\n",
    "<br> Suggester - 작업별 추천 방법 / 제안 우선순위 정렬 / 제안 이유 설명 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1615292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\n",
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "\n",
    "def load_image(image_path: str) -> np.ndarray:\n",
    "    if isinstance(image_path, np.ndarray):\n",
    "        return image_path\n",
    "    if image_path.startswith((\"http\", \"https\")):\n",
    "        _, image_suffix = os.path.splitext(image_path)\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=image_suffix) as tmp_file:\n",
    "            # Download the image and save it to the temporary file\n",
    "            with urllib.request.urlopen(image_path) as response:\n",
    "                tmp_file.write(response.read())\n",
    "            image_path = tmp_file.name\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return np.array(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9194fb",
   "metadata": {},
   "source": [
    "detection 입력 형식\n",
    "<br> Sequence[Message] = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Find tomatoes\",\n",
    "    \"media\": [\"img1.png\", \"img2.png\"]\n",
    "}\n",
    "\n",
    "LLM 입력 형식\n",
    "<br> fixed_chat = {\n",
    "    \"role\": msg[\"role\"],\n",
    "    \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"Find tomatoes.\"},\n",
    "        {\"type\": \"image_base64\", \"image_base64\": \"...\"},\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39c1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import base64\n",
    "import numpy as np\n",
    "\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from typing import Optional, Union\n",
    "\n",
    "\n",
    "def image_to_base64(image: Image.Image, resize: Optional[int] = None) -> str:\n",
    "    if resize is not None:\n",
    "        image.thumbnail((resize, resize))\n",
    "    buffer = BytesIO()\n",
    "    image.convert(\"RGB\").save(buffer, format=\"PNG\")\n",
    "    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def encode_media(\n",
    "    media: Union[str, Path, np.ndarray, Image.Image],\n",
    "    resize: Optional[int] = None,\n",
    ") -> str:\n",
    "    if isinstance(media, np.ndarray):\n",
    "        return image_to_base64(Image.fromarray(media), resize)\n",
    "\n",
    "    if isinstance(media, Image.Image):\n",
    "        return image_to_base64(media, resize)\n",
    "\n",
    "    if isinstance(media, (str, Path)):\n",
    "        path = Path(media)\n",
    "        suffix = path.suffix.lower()\n",
    "\n",
    "        if suffix in {\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\"}:\n",
    "            return image_to_base64(Image.open(path), resize)\n",
    "\n",
    "    raise ValueError(f\"Unsupported media type: {media}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eba721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from pathlib import Path\n",
    "from typing import Any, Iterator, Optional, Sequence, Union, TypedDict\n",
    "\n",
    "\n",
    "class Message(TypedDict, total=False):\n",
    "    role: str\n",
    "    content: str\n",
    "    media: Sequence[Union[str, Path]]\n",
    "\n",
    "ReturnType = str | Iterator[str | None]\n",
    "\n",
    "class LMM(ABC):\n",
    "    @abstractmethod\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        media: Optional[Sequence[Union[str, Path]]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ReturnType:\n",
    "        \"\"\"Single-prompt interface (optionally with media).\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def chat(\n",
    "        self,\n",
    "        chat: Sequence[Message],\n",
    "        **kwargs: Any,\n",
    "    ) -> ReturnType:\n",
    "        \"\"\"Chat interface with role/content(+media) messages.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        input: str | Sequence[Message],\n",
    "        **kwargs: Any,\n",
    "    ) -> ReturnType:\n",
    "        \"\"\"Unified call interface: str -> generate, messages -> chat.\"\"\"\n",
    "        if isinstance(input, str):\n",
    "            return self.generate(input, **kwargs)\n",
    "        return self.chat(input, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6fa4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Class: AnthropicLMM, OpsenAILMM\n",
    "from typing import Any, Dict, Iterator, List, Optional, Sequence, Union, cast\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "\n",
    "class OpenAILMM(LMM):\n",
    "    # 초기화\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"gpt-4o-2024-05-13\",\n",
    "        api_key: Optional[str] = None,\n",
    "        max_tokens: int = 4096,\n",
    "        json_mode: bool = False,\n",
    "        image_size: int = 768,\n",
    "        image_detail: str = \"low\",\n",
    "        **kwargs: Any\n",
    "    ):\n",
    "        if not api_key:\n",
    "            self.client = OpenAI()\n",
    "        else:\n",
    "            self.client = OpenAI(api_key=api_key)\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.image_size = image_size\n",
    "        self.image_detail = image_detail\n",
    "        # o1 does not use max_tokens\n",
    "        if \"max_tokens\" not in kwargs and not (\n",
    "            model_name.startswith(\"o1\") or model_name.startswith(\"o3\")\n",
    "        ):\n",
    "            kwargs[\"max_tokens\"] = max_tokens\n",
    "        if json_mode:\n",
    "            kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    # 호출 인터페이스\n",
    "    def __call__(\n",
    "        self,\n",
    "        input: Union[str, Sequence[Message]],\n",
    "        **kwargs: Any\n",
    "    ) -> str | Iterator[str | None]:\n",
    "        if isinstance(input, str):\n",
    "            return self.generate(input, **kwargs)\n",
    "        return self.chat(input, **kwargs)\n",
    "    \n",
    "    # 응답 생성\n",
    "    def chat(\n",
    "        self,\n",
    "        chat: Sequence[Message],\n",
    "        **kwargs: Any\n",
    "    ) -> Union[str, Iterator[Optional[str]]]:\n",
    "        \"\"\"LLM 모델 과의 채팅\n",
    "        입력 파라미터의 형태\n",
    "            str: [{\"role\": \"user\", \"content\": \"Hello!\"}, ...]\n",
    "            multimodal:[{\"role\": \"user\", \"content\": \"Hello!\", \"media\": [\"image1.jpg\", ...]}, ...]\n",
    "        \"\"\"\n",
    "        fixed_chat = []\n",
    "        for msg in chat:\n",
    "            fixed_c = {\"role\": msg[\"role\"]}\n",
    "            fixed_c[\"content\"] = [{\"type\": \"text\", \"text\": msg[\"content\"]}]\n",
    "        \n",
    "            if msg.get(\"media\") is not None and self.model_name != \"o3-mini\":\n",
    "                for media in msg[\"media\"]:\n",
    "                    resize = kwargs[\"resize\"] if \"resize\" in kwargs else self.image_size\n",
    "                    image_detail = kwargs.get(\"image_detail\", self.image_detail)\n",
    "                    encoded_media = encode_media(cast(str, media), resize=resize)\n",
    "\n",
    "                    fixed_c[\"content\"].append(\n",
    "                        {\n",
    "                            \"type\": \"image_base64\",\n",
    "                            \"image_base64\": encoded_media,\n",
    "                            \"detail\": image_detail, \n",
    "                        }\n",
    "                    )\n",
    "            fixed_chat.append(fixed_c)\n",
    "\n",
    "        tmp_kwargs = self.kwargs | kwargs  \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name, messages=fixed_chat, **tmp_kwargs \n",
    "        )\n",
    "        if \"stream\" in tmp_kwargs and tmp_kwargs[\"stream\"]:\n",
    "\n",
    "            def f() -> Iterator[Optional[str]]:\n",
    "                for chunk in response:\n",
    "                    chunk_message = chunk.choices[0].delta.content\n",
    "                    yield chunk_message\n",
    "\n",
    "            return f()\n",
    "        else:\n",
    "            return cast(str, response.choices[0].message.content)\n",
    "                \n",
    "    # 코드 생성\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        media: Optional[Sequence[Union[str, Path]]] = None,\n",
    "        **kwargs: Any\n",
    "    ) -> Union[str, Iterator[Optional[str]]]:\n",
    "        message: List[Dict[str, Any]] = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}]\n",
    "        if media and len(media) > 0 and self.model_name != \"o3-mini\":\n",
    "            for m in media:\n",
    "                resize = kwargs.get(\"resize\")\n",
    "                image_detail = kwargs.get(\"image_detail\", self.image_detail)\n",
    "                encoded_media = encode_media(m, resize=resize)\n",
    "\n",
    "                message[0][\"content\"].append(\n",
    "                    {\n",
    "                        \"type\": \"image_base64\",\n",
    "                        \"image_base64\": encoded_media,\n",
    "                        \"detail\": image_detail,\n",
    "                    }\n",
    "                )\n",
    "        # prefers kwargs from second dictionary over first\n",
    "        tmp_kwargs = self.kwargs | kwargs\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=message,\n",
    "            **tmp_kwargs,\n",
    "        )\n",
    "\n",
    "        if tmp_kwargs.get(\"stream\", False):\n",
    "\n",
    "            def f() -> Iterator[Optional[str]]:\n",
    "                for chunk in response:\n",
    "                    yield chunk.choices[0].delta.content \n",
    "            return f()\n",
    "        return cast(str, response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16289049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterator, List, Optional, Sequence, Union, cast\n",
    "import anthropic\n",
    "from anthropic.types import ImageBlockParam, MessageParam, TextBlockParam, ThinkingBlockParam\n",
    "\n",
    "class AnthropicLMM(LMM):\n",
    "    \"\"\"An LMM class for Anthropic models (base64-only image handling).\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: Optional[str] = None,\n",
    "        model_name: str = \"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens: int = 4096,\n",
    "        image_size: int = 768,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        self.client = anthropic.Anthropic(api_key=api_key)\n",
    "        self.model_name = model_name\n",
    "        self.image_size = image_size\n",
    "\n",
    "        if \"max_tokens\" not in kwargs:\n",
    "            kwargs[\"max_tokens\"] = max_tokens\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        input: Union[str, Sequence[Message]],\n",
    "        **kwargs: Any,\n",
    "    ) -> str | Iterator[str | None]:\n",
    "        if isinstance(input, str):\n",
    "            return self.generate(input, **kwargs)\n",
    "        return self.chat(input, **kwargs)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Internal helpers\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def _setup_chat_kwargs(\n",
    "        self, kwargs: Dict[str, Any]\n",
    "    ) -> tuple[Dict[str, Any], bool]:\n",
    "        tmp_kwargs = self.kwargs | kwargs\n",
    "        thinking_enabled = (\n",
    "            \"thinking\" in tmp_kwargs\n",
    "            and tmp_kwargs[\"thinking\"].get(\"type\") == \"enabled\"\n",
    "        )\n",
    "        if thinking_enabled:\n",
    "            tmp_kwargs[\"temperature\"] = 1.0\n",
    "        return tmp_kwargs, thinking_enabled\n",
    "\n",
    "    def _convert_messages(\n",
    "        self,\n",
    "        chat: Sequence[Message],\n",
    "        thinking_enabled: bool,\n",
    "        **kwargs: Any,\n",
    "    ) -> List[MessageParam]:\n",
    "        messages: List[MessageParam] = []\n",
    "\n",
    "        for msg in chat:\n",
    "            role = msg[\"role\"]\n",
    "\n",
    "            if role == \"user\":\n",
    "                content: List[Union[TextBlockParam, ImageBlockParam]] = [\n",
    "                    TextBlockParam(type=\"text\", text=cast(str, msg[\"content\"]))\n",
    "                ]\n",
    "\n",
    "                if msg.get(\"media\"):\n",
    "                    for media_path in msg[\"media\"]:\n",
    "                        resize = kwargs.get(\"resize\", self.image_size)\n",
    "                        encoded = encode_media(cast(str, media_path), resize=resize)\n",
    "\n",
    "                        # encode_media는 base64를 반환한다고 가정\n",
    "                        content.append(\n",
    "                            ImageBlockParam(\n",
    "                                type=\"image\",\n",
    "                                source={\n",
    "                                    \"type\": \"base64\",\n",
    "                                    \"media_type\": \"image/png\",\n",
    "                                    \"data\": encoded,\n",
    "                                },\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                messages.append(MessageParam(role=\"user\", content=content))\n",
    "\n",
    "            elif role == \"assistant\":\n",
    "                if thinking_enabled:\n",
    "                    messages.append(\n",
    "                        self._create_thinking_message(cast(str, msg[\"content\"]))\n",
    "                    )\n",
    "                else:\n",
    "                    messages.append(\n",
    "                        MessageParam(\n",
    "                            role=\"assistant\",\n",
    "                            content=[\n",
    "                                TextBlockParam(\n",
    "                                    type=\"text\", text=cast(str, msg[\"content\"])\n",
    "                                )\n",
    "                            ],\n",
    "                        )\n",
    "                    )\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported role: {role}\")\n",
    "\n",
    "        return messages\n",
    "\n",
    "    def _create_thinking_message(self, text: str) -> MessageParam:\n",
    "        content: List[Union[TextBlockParam, ThinkingBlockParam]] = []\n",
    "\n",
    "        thinking = extract_tag(text, \"thinking\")\n",
    "        signature = extract_tag(text, \"signature\")\n",
    "\n",
    "        if thinking:\n",
    "            content.append(\n",
    "                ThinkingBlockParam(\n",
    "                    type=\"thinking\",\n",
    "                    thinking=thinking.strip(),\n",
    "                    signature=signature.strip() if signature else \"\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        clean_text = text\n",
    "        if thinking:\n",
    "            clean_text = clean_text.replace(f\"<thinking>{thinking}</thinking>\", \"\")\n",
    "        if signature:\n",
    "            clean_text = clean_text.replace(f\"<signature>{signature}</signature>\", \"\")\n",
    "\n",
    "        if clean_text.strip():\n",
    "            content.append(TextBlockParam(type=\"text\", text=clean_text.strip()))\n",
    "\n",
    "        return MessageParam(role=\"assistant\", content=content)\n",
    "\n",
    "    def _handle_stream(\n",
    "        self,\n",
    "        stream: anthropic.Stream[anthropic.MessageStreamEvent],\n",
    "    ) -> Iterator[Optional[str]]:\n",
    "        def f() -> Iterator[Optional[str]]:\n",
    "            for chunk in stream:\n",
    "                if chunk.type == \"content_block_delta\":\n",
    "                    if hasattr(chunk.delta, \"text\"):\n",
    "                        yield chunk.delta.text\n",
    "                elif chunk.type == \"message_stop\":\n",
    "                    yield None\n",
    "\n",
    "        return f()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Public APIs\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def chat(\n",
    "        self,\n",
    "        chat: Sequence[Message],\n",
    "        **kwargs: Any,\n",
    "    ) -> Union[str, Iterator[Optional[str]]]:\n",
    "        tmp_kwargs, thinking_enabled = self._setup_chat_kwargs(kwargs)\n",
    "        messages = self._convert_messages(chat, thinking_enabled, **kwargs)\n",
    "\n",
    "        response = self.client.messages.create(\n",
    "            model=self.model_name,\n",
    "            messages=messages,\n",
    "            **tmp_kwargs,\n",
    "        )\n",
    "\n",
    "        if tmp_kwargs.get(\"stream\", False):\n",
    "            return self._handle_stream(\n",
    "                cast(anthropic.Stream[anthropic.MessageStreamEvent], response)\n",
    "            )\n",
    "\n",
    "        # non-streaming\n",
    "        msg = cast(anthropic.types.Message, response)\n",
    "        return cast(anthropic.types.TextBlock, msg.content[-1]).text\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        media: Optional[Sequence[Union[str, Path]]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Union[str, Iterator[Optional[str]]]:\n",
    "        content: List[Union[TextBlockParam, ImageBlockParam]] = [\n",
    "            TextBlockParam(type=\"text\", text=prompt)\n",
    "        ]\n",
    "\n",
    "        if media:\n",
    "            for m in media:\n",
    "                resize = kwargs.get(\"resize\", self.image_size)\n",
    "                encoded = encode_media(cast(str, m), resize=resize)\n",
    "\n",
    "                content.append(\n",
    "                    ImageBlockParam(\n",
    "                        type=\"image\",\n",
    "                        source={\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/png\",\n",
    "                            \"data\": encoded,\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        tmp_kwargs = self.kwargs | kwargs\n",
    "        response = self.client.messages.create(\n",
    "            model=self.model_name,\n",
    "            messages=[MessageParam(role=\"user\", content=content)],\n",
    "            **tmp_kwargs,\n",
    "        )\n",
    "\n",
    "        if tmp_kwargs.get(\"stream\", False):\n",
    "\n",
    "            def f() -> Iterator[Optional[str]]:\n",
    "                for chunk in response:\n",
    "                    if chunk.type == \"content_block_delta\":\n",
    "                        yield chunk.delta.text\n",
    "                    elif chunk.type == \"message_stop\":\n",
    "                        yield None\n",
    "\n",
    "            return f()\n",
    "\n",
    "        return cast(anthropic.types.TextBlock, response.content[-1]).text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0095eaf9",
   "metadata": {},
   "source": [
    "확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81bac0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMM Configuration Complete\n",
      "  - Analyzer: claude-sonnet-4-5-20250929\n",
      "  - Planner: claude-sonnet-4-5-20250929\n",
      "  - VQA: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Step 1: LMM Configuration & Initialization\n",
    "# ========================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "env_path = Path.cwd().parent / \".env\"\n",
    "load_dotenv(env_path, override=True)\n",
    "\n",
    "\n",
    "# config \n",
    "# Initialize different LMMs for different roles\n",
    "analyzer_lmm = AnthropicLMM(\n",
    "    model_name=\"claude-sonnet-4-5-20250929\",\n",
    "    temperature=0.0,  # Precise analysis\n",
    "    max_tokens=2048\n",
    ")\n",
    "\n",
    "planner_lmm = AnthropicLMM(\n",
    "    model_name=\"claude-sonnet-4-5-20250929\",\n",
    "    temperature=0.0,  # Precise planning\n",
    "    max_tokens=4096\n",
    ")\n",
    "\n",
    "vqa_lmm = OpenAILMM(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.0,  # Fast image understanding\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "print(\"LMM Configuration Complete\")\n",
    "print(f\"  - Analyzer: {analyzer_lmm.model_name}\")\n",
    "print(f\"  - Planner: {planner_lmm.model_name}\")\n",
    "print(f\"  - VQA: {vqa_lmm.model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fea7a164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "User Request Analysis\n",
      "======================================================================\n",
      "Request: 이 이미지에 있는 토마토를 감지하고 개수를 세어주세요\n",
      "\n",
      "Using LMM type: AnthropicLMM\n",
      "Analysis Result:\n",
      "  task_type: detection|counting\n",
      "  main_objects: ['tomato']\n",
      "  task_complexity: simple\n",
      "  requirements: ['detect all tomatoes in image', 'count total number of tomatoes']\n",
      "  suggested_tools: ['object_detection', 'instance_segmentation', 'counting_algorithm']\n",
      "  reasoning: The user is asking to detect tomatoes in an image and count them. This is a two-step task that requires: 1) Object detection to locate all tomatoes in the image, and 2) Counting the detected instances. This is a straightforward computer vision task that can be accomplished using standard object detection models (like YOLO, Faster R-CNN) or instance segmentation models that can identify individual tomato instances and provide an accurate count.\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Step 2: User Request Analyzer\n",
    "# ========================================\n",
    "\n",
    "ANALYZE_PROMPT = \"\"\"**Role**: You are an expert request analyzer for vision AI tasks.\n",
    "\n",
    "**Task**: Analyze the user's request and break it down into key components that will help in planning and execution.\n",
    "\n",
    "**User Request**: {user_request}\n",
    "\n",
    "**Instructions**:\n",
    "1. Identify the main task (detection, counting, classification, segmentation, etc.)\n",
    "2. Extract key objects or concepts mentioned\n",
    "3. Determine if the task requires single or multiple steps\n",
    "4. Identify any specific constraints or requirements\n",
    "5. Suggest the type of vision tools that might be needed\n",
    "\n",
    "**Output Format**:\n",
    "Provide your analysis in the following JSON format:\n",
    "{{\n",
    "    \"task_type\": \"detection|counting|segmentation|classification|tracking|other\",\n",
    "    \"main_objects\": [\"object1\", \"object2\"],\n",
    "    \"task_complexity\": \"simple|moderate|complex\",\n",
    "    \"requirements\": [\"requirement1\", \"requirement2\"],\n",
    "    \"suggested_tools\": [\"tool1\", \"tool2\"],\n",
    "    \"reasoning\": \"Your explanation here\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "def analyze_user_request(user_request: str, lmm) -> dict:\n",
    "    \"\"\"사용자 요청을 분석하여 구조화된 정보 반환\"\"\"\n",
    "    \n",
    "    prompt = ANALYZE_PROMPT.format(user_request=user_request)\n",
    "    \n",
    "    # FIX: LMM 타입에 따라 다르게 처리\n",
    "    print(f\"Using LMM type: {type(lmm).__name__}\")\n",
    "    \n",
    "    if isinstance(lmm, OpenAILMM):\n",
    "        # OpenAI는 response_format 지원\n",
    "        response = lmm.generate(\n",
    "            prompt, \n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "    elif isinstance(lmm, AnthropicLMM):\n",
    "        # Anthropic은 response_format 미지원, 프롬프트에 명시\n",
    "        json_instruction = \"\\n\\nYou MUST respond with ONLY a valid JSON object, no additional text.\"\n",
    "        response = lmm.generate(prompt + json_instruction)\n",
    "    else:\n",
    "        # 기타 LMM\n",
    "        response = lmm.generate(prompt)\n",
    "    \n",
    "    try:\n",
    "        import json\n",
    "        # JSON 추출 (코드 블록 안에 있을 수 있음)\n",
    "        response_str = response.strip()\n",
    "        if response_str.startswith(\"```json\"):\n",
    "            response_str = response_str[7:-3].strip()\n",
    "        elif response_str.startswith(\"```\"):\n",
    "            response_str = response_str[3:-3].strip()\n",
    "        \n",
    "        analysis = json.loads(response_str)\n",
    "        return analysis\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Failed to parse response: {e}\", \"raw_response\": response}\n",
    "\n",
    "# Test the analyzer\n",
    "test_request = \"이 이미지에 있는 토마토를 감지하고 개수를 세어주세요\"\n",
    "print(\"=\" * 70)\n",
    "print(\"User Request Analysis\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Request: {test_request}\\n\")\n",
    "\n",
    "analysis = analyze_user_request(test_request, analyzer_lmm)\n",
    "print(\"Analysis Result:\")\n",
    "for key, value in analysis.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90eee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# planner tools\n",
    "def vaq():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0643f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# planner tools\n",
    "def suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a1e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e88344",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
