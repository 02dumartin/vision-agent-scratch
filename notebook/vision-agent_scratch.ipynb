{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d79166c",
   "metadata": {},
   "source": [
    "### Vision Detection Agent scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b116f0a",
   "metadata": {},
   "source": [
    "⸻"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0206ef7d",
   "metadata": {},
   "source": [
    "### 10 step\n",
    "\n",
    "* [1-3 step]: 이해 및 계획\n",
    "<br> 사용자 요청 분석 → 이미지 로드: VQA(Visual Question Answering)로 이미지 이해\n",
    "\n",
    "* [4-6 step]: 도구 선택\n",
    "<br> suggestion() → get_tool_for_task() → 최적 도구 결정\n",
    "\n",
    "* [7-9 step]: 실행 및 검증\n",
    "\n",
    "* [10 step]: 최종 코드 생성 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c6bd05",
   "metadata": {},
   "source": [
    "#### [1-3 step]: 이해 및 계획\n",
    "ImageLoader - 이미지 파일 로드 / 이미지 리사이즈 / PIL to Numpy / 유효성\n",
    "<br> VQAModel - 이미지에 대한 질문 / 전체 설명 / 특정 객체 개수 추정\n",
    "<br> Planner - LMM 초기화/ 사용자 요청 분석 / 단계별 계획 생성/ 계획 to markdown / 검증\n",
    "<br> Suggester - 작업별 추천 방법 / 제안 우선순위 정렬 / 제안 이유 설명 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8a145d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import tempfile\n",
    "import base64\n",
    "\n",
    "import abc\n",
    "import base64\n",
    "import logging\n",
    "import os\n",
    "import platform\n",
    "import re\n",
    "import sys\n",
    "import traceback\n",
    "import warnings\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "from typing import Any, Dict, Iterable, List, Optional, Union\n",
    "\n",
    "import nbformat\n",
    "from dotenv import load_dotenv\n",
    "from nbclient import NotebookClient\n",
    "from nbclient import __version__ as nbclient_version\n",
    "from nbclient.exceptions import CellTimeoutError, DeadKernelError\n",
    "from nbclient.util import run_sync\n",
    "from nbformat.v4 import new_code_cell\n",
    "from opentelemetry.context import get_current\n",
    "from opentelemetry.trace import SpanKind, Status, StatusCode, get_tracer\n",
    "from pydantic import BaseModel, field_serializer\n",
    "from typing_extensions import Self\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union, Optional, cast , Dict, Any\n",
    "from pydantic import BaseModel\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from PIL.Image import Image as ImageTypes\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda7a99",
   "metadata": {},
   "source": [
    "* 입력 표준화\n",
    "<br> AgentMessage(content, media) 만들고 내부 형식으로 변환 / 텍스트와 이미지의 묶음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040572ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Literal, Optional, Union\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class AgentMessage(BaseModel):\n",
    "    role: Union[\n",
    "        Literal[\"user\"],\n",
    "        Literal[\"assistant\"],  # planner, coder and conversation are of type assistant\n",
    "        Literal[\"observation \"],\n",
    "        Literal[\"final_observation\"],  # the observation from the final code output\n",
    "        Literal[\"error_observation\"],  # the observation from the error message\n",
    "        Literal[\"interaction\"],\n",
    "        Literal[\"interaction_response\"],\n",
    "        Literal[\"conversation\"],\n",
    "        Literal[\"planner\"],\n",
    "        Literal[\n",
    "            \"planner_update\"\n",
    "        ],  # an intermediate update from the planner to show partial information\n",
    "        Literal[\"coder\"],\n",
    "    ]\n",
    "    content: str\n",
    "    media: Optional[List[Union[str, Path]]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38c31a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/utils/types.py\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Logs:\n",
    "    stdout: List[str] = field(default_factory=list)\n",
    "    stderr: List[str] = field(default_factory=list)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Error:\n",
    "    name: str\n",
    "    value: str\n",
    "    traceback_raw: List[str] = field(default_factory=list)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Result:\n",
    "    is_main_result: bool\n",
    "    data: Dict[str, Any]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Execution:\n",
    "    logs: Logs = field(default_factory=Logs)\n",
    "    results: List[Result] = field(default_factory=list)\n",
    "    error: Optional[Error] = None\n",
    "\n",
    "    @property\n",
    "    def ok(self) -> bool:\n",
    "        return self.error is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2dd02937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # src/utils/excute.py\n",
    "# 표준화 유틸\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "load_dotenv()\n",
    "_LOGGER = logging.getLogger(__name__)\n",
    "_SESSION_TIMEOUT = 600  # 10 minutes\n",
    "WORKSPACE = Path(os.getenv(\"WORKSPACE\", \"\"))\n",
    "\n",
    "class CodeInterpreter(abc.ABC):\n",
    "    \n",
    "    \"\"\"Code interpreter interface.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        timeout: int,\n",
    "        remote_path: Optional[Union[str, Path]] = None,\n",
    "        non_exiting: bool = False,\n",
    "        *args: Any,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        self.timeout = timeout\n",
    "        self.remote_path = Path(remote_path if remote_path is not None else WORKSPACE)\n",
    "        self.non_exiting = non_exiting\n",
    "\n",
    "    def __enter__(self) -> Self:\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *exc_info: Any) -> None:\n",
    "        if not self.non_exiting:\n",
    "            self.close()\n",
    "\n",
    "    def close(self, *args: Any, **kwargs: Any) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def restart_kernel(self) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def exec_cell(self, code: str) -> Execution:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def exec_isolation(self, code: str) -> Execution:\n",
    "        self.restart_kernel()\n",
    "        return self.exec_cell(code)\n",
    "\n",
    "    def upload_file(self, file: Union[str, Path]) -> Path:\n",
    "        # Default behavior is a no-op (for local code interpreter)\n",
    "        return Path(file)\n",
    "\n",
    "    def download_file(\n",
    "        self, remote_file_path: Union[str, Path], local_file_path: Union[str, Path]\n",
    "    ) -> Path:\n",
    "        # Default behavior is a no-op (for local code interpreter)\n",
    "        return Path(local_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81a6e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준화 유틸\n",
    "# # src/utils/excute.py\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "load_dotenv()\n",
    "_LOGGER = logging.getLogger(__name__)\n",
    "_SESSION_TIMEOUT = 600  # 10 minutes\n",
    "WORKSPACE = Path(os.getenv(\"WORKSPACE\", \"\"))\n",
    "\n",
    "class CodeInterpreterFactory:\n",
    "    \"\"\"Factory class for creating code interpreters.\n",
    "    Could be extended to support multiple code interpreters.\n",
    "    \"\"\"\n",
    "\n",
    "    _instance_map: Dict[str, CodeInterpreter] = {}\n",
    "    _default_key = \"default\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_instance() -> CodeInterpreter:\n",
    "        warnings.warn(\n",
    "            \"Use new_instance() instead for production usage, get_default_instance() is for testing and will be removed in the future.\"\n",
    "        )\n",
    "        inst_map = CodeInterpreterFactory._instance_map\n",
    "        instance = inst_map.get(CodeInterpreterFactory._default_key)\n",
    "        if instance:\n",
    "            return instance\n",
    "        instance = CodeInterpreterFactory.new_instance()\n",
    "        inst_map[CodeInterpreterFactory._default_key] = instance\n",
    "        return instance\n",
    "\n",
    "    @staticmethod\n",
    "    def new_instance(\n",
    "        code_sandbox_runtime: Optional[str] = None,\n",
    "        remote_path: Optional[Union[str, Path]] = None,\n",
    "        non_exiting: bool = False,\n",
    "    ) -> CodeInterpreter:\n",
    "        if not code_sandbox_runtime:\n",
    "            code_sandbox_runtime = os.getenv(\"CODE_SANDBOX_RUNTIME\", \"local\")\n",
    "        if code_sandbox_runtime == \"local\":\n",
    "            instance = LocalCodeInterpreter(\n",
    "                timeout=_SESSION_TIMEOUT,\n",
    "                remote_path=remote_path,\n",
    "                non_exiting=non_exiting,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported code sandbox runtime: {code_sandbox_runtime}. Supported runtimes: local\"\n",
    "            )\n",
    "        return instance\n",
    "\n",
    "\n",
    "def _parse_local_code_interpreter_outputs(outputs: List[Dict[str, Any]]) -> Execution:\n",
    "    \"\"\"Parse notebook cell outputs to Execution object. Output types:\n",
    "    https://nbformat.readthedocs.io/en/latest/format_description.html#code-cell-outputs\n",
    "    \"\"\"\n",
    "    execution = Execution()\n",
    "    for data in outputs:\n",
    "        if data[\"output_type\"] == \"error\":\n",
    "            _LOGGER.debug(\"Cell finished execution with error\")\n",
    "            execution.error = Error(\n",
    "                name=data[\"ename\"],\n",
    "                value=data[\"evalue\"],\n",
    "                traceback_raw=data[\"traceback\"],\n",
    "            )\n",
    "        elif data[\"output_type\"] == \"stream\":\n",
    "            if data[\"name\"] == \"stdout\":\n",
    "                execution.logs.stdout.append(data[\"text\"])\n",
    "            elif data[\"name\"] == \"stderr\":\n",
    "                execution.logs.stderr.append(data[\"text\"])\n",
    "        elif data[\"output_type\"] in \"display_data\":\n",
    "            result = Result(is_main_result=False, data=data[\"data\"])\n",
    "            execution.results.append(result)\n",
    "        elif data[\"output_type\"] == \"execute_result\":\n",
    "            result = Result(is_main_result=True, data=data[\"data\"])\n",
    "            execution.results.append(result)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown output type: {data['output_type']}\")\n",
    "    return execution\n",
    "\n",
    "def _remove_escape_and_color_codes(input_str: str) -> str:\n",
    "    pattern = re.compile(r\"\\x1b\\[[0-9;]*[mK]\")\n",
    "    return pattern.sub(\"\", input_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c569976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준화 유틸 \n",
    "\n",
    "def b64_to_pil(b64_str: str) -> ImageTypes:\n",
    "    if \",\" in b64_str:\n",
    "        b64_str = b64_str.split(\",\")[1]\n",
    "    return Image.open(BytesIO(base64.b64decode(b64_str)))\n",
    "\n",
    "def add_media_to_chat(\n",
    "    chat: List[AgentMessage],\n",
    "    code_interpreter: Optional[CodeInterpreter] = None,\n",
    "    append_to_prompt: bool = True,\n",
    ") -> Tuple[List[AgentMessage], List[AgentMessage], List[Union[str, Path]]]:\n",
    "    orig_chat = copy.deepcopy(chat)\n",
    "    int_chat = copy.deepcopy(chat)\n",
    "    media_list: List[Union[str, Path]] = []\n",
    "    \n",
    "    for chat_i in int_chat:\n",
    "        if chat_i.media is not None:\n",
    "            media_list_i: List[Union[str, Path]] = []\n",
    "            for media in chat_i.media:\n",
    "                if isinstance(media, str) and media.startswith(\"data:image/\"):\n",
    "                    media_pil = b64_to_pil(media)\n",
    "                    with tempfile.NamedTemporaryFile(\n",
    "                        mode=\"wb\", suffix=\".png\", delete=False\n",
    "                    ) as temp_file:\n",
    "                        media_pil.save(temp_file, format=\"PNG\")\n",
    "                        media = str(temp_file.name)\n",
    "                elif isinstance(media, str) and media.startswith(\"data:video/\"):\n",
    "                    ext = media.split(\";\")[0].split(\"/\")[-1]\n",
    "                    with tempfile.NamedTemporaryFile(\n",
    "                        mode=\"wb\", suffix=f\".{ext}\", delete=False\n",
    "                    ) as temp_file:\n",
    "                        media_bytes = base64.b64decode(media.split(\",\")[1])\n",
    "                        temp_file.write(media_bytes)\n",
    "                        media = str(temp_file.name)\n",
    "                if code_interpreter is not None:\n",
    "                    media = str(code_interpreter.upload_file(media))\n",
    "                media_list_i.append(media)\n",
    "                # don't duplicate appending media name and only add them for user messages\n",
    "                if (\n",
    "                    not str(chat_i.content).endswith(f\" Media name {media}\")\n",
    "                    and chat_i.role == \"user\"\n",
    "                    and append_to_prompt\n",
    "                ):\n",
    "                    chat_i.content += f\" Media name {media}\"\n",
    "            chat_i.media = media_list_i if len(media_list_i) > 0 else None\n",
    "            media_list.extend(media_list_i)\n",
    "\n",
    "    int_chat = cast(\n",
    "        List[AgentMessage],\n",
    "        [\n",
    "            (\n",
    "                AgentMessage(\n",
    "                    role=c.role,\n",
    "                    content=c.content,\n",
    "                    media=c.media,\n",
    "                )\n",
    "                if c.media is not None\n",
    "                else AgentMessage(role=c.role, content=c.content, media=None)\n",
    "            )\n",
    "            for c in int_chat\n",
    "        ],\n",
    "    )\n",
    "    return int_chat, orig_chat, media_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b5077bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from nbclient import NotebookClient\n",
    "\n",
    "class LocalCodeInterpreter(CodeInterpreter):\n",
    "    def __init__(\n",
    "        self,\n",
    "        timeout: int,\n",
    "        remote_path: Optional[Union[str, Path]] = None,\n",
    "        non_exiting: bool = False,\n",
    "        kernel_name: str = \"python3\",\n",
    "    ):\n",
    "        super().__init__(timeout=timeout, remote_path=remote_path, non_exiting=non_exiting)\n",
    "        self.kernel_name = kernel_name\n",
    "        self.nb = nbformat.v4.new_notebook()\n",
    "        self.client = NotebookClient(\n",
    "            self.nb,\n",
    "            timeout=self.timeout,\n",
    "            kernel_name=self.kernel_name,\n",
    "            allow_errors=True,\n",
    "        )\n",
    "        # 커널 시작\n",
    "        self.client.create_kernel_manager()\n",
    "        self.client.start_new_kernel()\n",
    "\n",
    "    def close(self, *args: Any, **kwargs: Any) -> None:\n",
    "        try:\n",
    "            self.client.shutdown_kernel(now=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def restart_kernel(self) -> None:\n",
    "        # 커널 재시작 + notebook reset\n",
    "        try:\n",
    "            self.client.shutdown_kernel(now=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "        self.nb = nbformat.v4.new_notebook()\n",
    "        self.client = NotebookClient(\n",
    "            self.nb,\n",
    "            timeout=self.timeout,\n",
    "            kernel_name=self.kernel_name,\n",
    "            allow_errors=True,\n",
    "        )\n",
    "        self.client.create_kernel_manager()\n",
    "        self.client.start_new_kernel()\n",
    "\n",
    "    def exec_cell(self, code: str) -> Execution:\n",
    "        cell = nbformat.v4.new_code_cell(code)\n",
    "        self.nb.cells.append(cell)\n",
    "        idx = len(self.nb.cells) - 1\n",
    "\n",
    "        # 셀 실행\n",
    "        self.client.execute_cell(cell, idx)\n",
    "\n",
    "        outputs = cell.get(\"outputs\", []) or []\n",
    "        return _parse_local_code_interpreter_outputs(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07299673",
   "metadata": {},
   "source": [
    "* 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac5482e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== orig_chat (deep copied original) =====\n",
      "\n",
      "[orig_chat 0]\n",
      "role   : user\n",
      "content: 이 이미지에 있는 토마토를 감지하고 개수를 세어주세요.\n",
      "media  : ['data\\\\image.png']\n",
      "\n",
      "===== int_chat (internal standardized chat) =====\n",
      "\n",
      "[int_chat 0]\n",
      "role   : user\n",
      "content: 이 이미지에 있는 토마토를 감지하고 개수를 세어주세요. Media name data\\image.png\n",
      "media  : ['data\\\\image.png']\n",
      "\n",
      "int_chat last message keys: dict_keys(['role', 'content', 'media'])\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# env 로드\n",
    "env_path = Path.cwd().parent / \".env\"\n",
    "load_dotenv(env_path, override=True)\n",
    "\n",
    "# 입력 표준화 \n",
    "user_request = \"이 이미지에 있는 토마토를 감지하고 개수를 세어주세요.\"\n",
    "image_path = Path(\"data/image.png\")\n",
    "\n",
    "\n",
    "chat = [\n",
    "    AgentMessage(\n",
    "        role=\"user\",\n",
    "        content=user_request,\n",
    "        media=[str(image_path)],   # 이미지 경로를 media로 넣기\n",
    "    )\n",
    "]\n",
    "\n",
    "with CodeInterpreterFactory.new_instance(None) as code_interpreter:\n",
    "    int_chat, orig_chat, _ = add_media_to_chat(chat, code_interpreter)\n",
    "\n",
    "print(\"\\n===== orig_chat (deep copied original) =====\")\n",
    "for i, msg in enumerate(orig_chat):\n",
    "    print(f\"\\n[orig_chat {i}]\")\n",
    "    print(\"role   :\", msg.role)\n",
    "    print(\"content:\", msg.content)\n",
    "    print(\"media  :\", msg.media)\n",
    "\n",
    "print(\"\\n===== int_chat (internal standardized chat) =====\")\n",
    "for i, msg in enumerate(int_chat):\n",
    "    print(f\"\\n[int_chat {i}]\")\n",
    "    print(\"role   :\", msg.role)\n",
    "    print(\"content:\", msg.content)\n",
    "    print(\"media  :\", msg.media)\n",
    "\n",
    "print(\"\\nint_chat last message keys:\", int_chat[-1].model_dump().keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6402386d",
   "metadata": {},
   "source": [
    "* Planner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9194fb",
   "metadata": {},
   "source": [
    "detection 입력 형식\n",
    "<br> Sequence[Message] = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Find tomatoes\",\n",
    "    \"media\": [\"img1.png\", \"img2.png\"]\n",
    "}\n",
    "\n",
    "LLM 입력 형식\n",
    "<br> fixed_chat = {\n",
    "    \"role\": msg[\"role\"],\n",
    "    \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"Find tomatoes.\"},\n",
    "        {\"type\": \"image_base64\", \"image_base64\": \"...\"},\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1615292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\n",
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "\n",
    "def load_image(image_path: str) -> np.ndarray:\n",
    "    if isinstance(image_path, np.ndarray):\n",
    "        return image_path\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39c1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import base64\n",
    "import numpy as np\n",
    "\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from typing import Optional, Union\n",
    "\n",
    "\n",
    "def image_to_base64(image: Image.Image, resize: Optional[int] = None) -> str:\n",
    "    if resize is not None:\n",
    "        image.thumbnail((resize, resize))\n",
    "    buffer = BytesIO()\n",
    "    image.convert(\"RGB\").save(buffer, format=\"PNG\")\n",
    "    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def encode_media(\n",
    "    media: Union[str, Path, np.ndarray, Image.Image],\n",
    "    resize: Optional[int] = None,\n",
    ") -> str:\n",
    "    if isinstance(media, np.ndarray):\n",
    "        return image_to_base64(Image.fromarray(media), resize)\n",
    "\n",
    "    if isinstance(media, Image.Image):\n",
    "        return image_to_base64(media, resize)\n",
    "\n",
    "    if isinstance(media, (str, Path)):\n",
    "        path = Path(media)\n",
    "        suffix = path.suffix.lower()\n",
    "\n",
    "        if suffix in {\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\"}:\n",
    "            return image_to_base64(Image.open(path), resize)\n",
    "\n",
    "    raise ValueError(f\"Unsupported media type: {media}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eba721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from pathlib import Path\n",
    "from typing import Any, Iterator, Optional, Sequence, Union, TypedDict\n",
    "\n",
    "\n",
    "class Message(TypedDict, total=False):\n",
    "    role: str\n",
    "    content: str\n",
    "    media: Sequence[Union[str, Path]]\n",
    "\n",
    "ReturnType = str | Iterator[str | None]\n",
    "\n",
    "class LMM(ABC):\n",
    "    @abstractmethod\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        media: Optional[Sequence[Union[str, Path]]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ReturnType:\n",
    "        \"\"\"Single-prompt interface (optionally with media).\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def chat(\n",
    "        self,\n",
    "        chat: Sequence[Message],\n",
    "        **kwargs: Any,\n",
    "    ) -> ReturnType:\n",
    "        \"\"\"Chat interface with role/content(+media) messages.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        input: str | Sequence[Message],\n",
    "        **kwargs: Any,\n",
    "    ) -> ReturnType:\n",
    "        \"\"\"Unified call interface: str -> generate, messages -> chat.\"\"\"\n",
    "        if isinstance(input, str):\n",
    "            return self.generate(input, **kwargs)\n",
    "        return self.chat(input, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6fa4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Class: AnthropicLMM, OpsenAILMM\n",
    "from typing import Any, Dict, Iterator, List, Optional, Sequence, Union, cast\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "\n",
    "class OpenAILMM(LMM):\n",
    "    # 초기화\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"gpt-4o-2024-05-13\",\n",
    "        api_key: Optional[str] = None,\n",
    "        max_tokens: int = 4096,\n",
    "        json_mode: bool = False,\n",
    "        image_size: int = 768,\n",
    "        image_detail: str = \"low\",\n",
    "        **kwargs: Any\n",
    "    ):\n",
    "        if not api_key:\n",
    "            self.client = OpenAI()\n",
    "        else:\n",
    "            self.client = OpenAI(api_key=api_key)\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.image_size = image_size\n",
    "        self.image_detail = image_detail\n",
    "        # o1 does not use max_tokens\n",
    "        if \"max_tokens\" not in kwargs and not (\n",
    "            model_name.startswith(\"o1\") or model_name.startswith(\"o3\")\n",
    "        ):\n",
    "            kwargs[\"max_tokens\"] = max_tokens\n",
    "        if json_mode:\n",
    "            kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    # 호출 인터페이스\n",
    "    def __call__(\n",
    "        self,\n",
    "        input: Union[str, Sequence[Message]],\n",
    "        **kwargs: Any\n",
    "    ) -> str | Iterator[str | None]:\n",
    "        if isinstance(input, str):\n",
    "            return self.generate(input, **kwargs)\n",
    "        return self.chat(input, **kwargs)\n",
    "    \n",
    "    # 응답 생성\n",
    "    def chat(\n",
    "        self,\n",
    "        chat: Sequence[Message],\n",
    "        **kwargs: Any\n",
    "    ) -> Union[str, Iterator[Optional[str]]]:\n",
    "        \"\"\"LLM 모델 과의 채팅\n",
    "        입력 파라미터의 형태\n",
    "            str: [{\"role\": \"user\", \"content\": \"Hello!\"}, ...]\n",
    "            multimodal:[{\"role\": \"user\", \"content\": \"Hello!\", \"media\": [\"image1.jpg\", ...]}, ...]\n",
    "        \"\"\"\n",
    "        fixed_chat = []\n",
    "        for msg in chat:\n",
    "            fixed_c = {\"role\": msg[\"role\"]}\n",
    "            fixed_c[\"content\"] = [{\"type\": \"text\", \"text\": msg[\"content\"]}]\n",
    "        \n",
    "            if msg.get(\"media\") is not None and self.model_name != \"o3-mini\":\n",
    "                for media in msg[\"media\"]:\n",
    "                    resize = kwargs[\"resize\"] if \"resize\" in kwargs else self.image_size\n",
    "                    image_detail = kwargs.get(\"image_detail\", self.image_detail)\n",
    "                    encoded_media = encode_media(cast(str, media), resize=resize)\n",
    "\n",
    "                    fixed_c[\"content\"].append(\n",
    "                        {\n",
    "                            \"type\": \"image_base64\",\n",
    "                            \"image_base64\": encoded_media,\n",
    "                            \"detail\": image_detail, \n",
    "                        }\n",
    "                    )\n",
    "            fixed_chat.append(fixed_c)\n",
    "\n",
    "        tmp_kwargs = self.kwargs | kwargs  \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name, messages=fixed_chat, **tmp_kwargs \n",
    "        )\n",
    "        if \"stream\" in tmp_kwargs and tmp_kwargs[\"stream\"]:\n",
    "\n",
    "            def f() -> Iterator[Optional[str]]:\n",
    "                for chunk in response:\n",
    "                    chunk_message = chunk.choices[0].delta.content\n",
    "                    yield chunk_message\n",
    "\n",
    "            return f()\n",
    "        else:\n",
    "            return cast(str, response.choices[0].message.content)\n",
    "                \n",
    "    # 코드 생성\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        media: Optional[Sequence[Union[str, Path]]] = None,\n",
    "        **kwargs: Any\n",
    "    ) -> Union[str, Iterator[Optional[str]]]:\n",
    "        message: List[Dict[str, Any]] = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}]\n",
    "        if media and len(media) > 0 and self.model_name != \"o3-mini\":\n",
    "            for m in media:\n",
    "                resize = kwargs.get(\"resize\")\n",
    "                image_detail = kwargs.get(\"image_detail\", self.image_detail)\n",
    "                encoded_media = encode_media(m, resize=resize)\n",
    "\n",
    "                message[0][\"content\"].append(\n",
    "                    {\n",
    "                        \"type\": \"image_base64\",\n",
    "                        \"image_base64\": encoded_media,\n",
    "                        \"detail\": image_detail,\n",
    "                    }\n",
    "                )\n",
    "        # prefers kwargs from second dictionary over first\n",
    "        tmp_kwargs = self.kwargs | kwargs\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=message,\n",
    "            **tmp_kwargs,\n",
    "        )\n",
    "\n",
    "        if tmp_kwargs.get(\"stream\", False):\n",
    "\n",
    "            def f() -> Iterator[Optional[str]]:\n",
    "                for chunk in response:\n",
    "                    yield chunk.choices[0].delta.content \n",
    "            return f()\n",
    "        return cast(str, response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16289049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterator, List, Optional, Sequence, Union, cast\n",
    "import anthropic\n",
    "from anthropic.types import ImageBlockParam, MessageParam, TextBlockParam, ThinkingBlockParam\n",
    "\n",
    "class AnthropicLMM(LMM):\n",
    "    \"\"\"An LMM class for Anthropic models (base64-only image handling).\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: Optional[str] = None,\n",
    "        model_name: str = \"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens: int = 4096,\n",
    "        image_size: int = 768,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        self.client = anthropic.Anthropic(api_key=api_key)\n",
    "        self.model_name = model_name\n",
    "        self.image_size = image_size\n",
    "\n",
    "        if \"max_tokens\" not in kwargs:\n",
    "            kwargs[\"max_tokens\"] = max_tokens\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        input: Union[str, Sequence[Message]],\n",
    "        **kwargs: Any,\n",
    "    ) -> str | Iterator[str | None]:\n",
    "        if isinstance(input, str):\n",
    "            return self.generate(input, **kwargs)\n",
    "        return self.chat(input, **kwargs)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Internal helpers\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def _setup_chat_kwargs(\n",
    "        self, kwargs: Dict[str, Any]\n",
    "    ) -> tuple[Dict[str, Any], bool]:\n",
    "        tmp_kwargs = self.kwargs | kwargs\n",
    "        thinking_enabled = (\n",
    "            \"thinking\" in tmp_kwargs\n",
    "            and tmp_kwargs[\"thinking\"].get(\"type\") == \"enabled\"\n",
    "        )\n",
    "        if thinking_enabled:\n",
    "            tmp_kwargs[\"temperature\"] = 1.0\n",
    "        return tmp_kwargs, thinking_enabled\n",
    "\n",
    "    def _convert_messages(\n",
    "        self,\n",
    "        chat: Sequence[Message],\n",
    "        thinking_enabled: bool,\n",
    "        **kwargs: Any,\n",
    "    ) -> List[MessageParam]:\n",
    "        messages: List[MessageParam] = []\n",
    "\n",
    "        for msg in chat:\n",
    "            role = msg[\"role\"]\n",
    "\n",
    "            if role == \"user\":\n",
    "                content: List[Union[TextBlockParam, ImageBlockParam]] = [\n",
    "                    TextBlockParam(type=\"text\", text=cast(str, msg[\"content\"]))\n",
    "                ]\n",
    "\n",
    "                if msg.get(\"media\"):\n",
    "                    for media_path in msg[\"media\"]:\n",
    "                        resize = kwargs.get(\"resize\", self.image_size)\n",
    "                        encoded = encode_media(cast(str, media_path), resize=resize)\n",
    "\n",
    "                        # encode_media는 base64를 반환한다고 가정\n",
    "                        content.append(\n",
    "                            ImageBlockParam(\n",
    "                                type=\"image\",\n",
    "                                source={\n",
    "                                    \"type\": \"base64\",\n",
    "                                    \"media_type\": \"image/png\",\n",
    "                                    \"data\": encoded,\n",
    "                                },\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                messages.append(MessageParam(role=\"user\", content=content))\n",
    "\n",
    "            elif role == \"assistant\":\n",
    "                if thinking_enabled:\n",
    "                    messages.append(\n",
    "                        self._create_thinking_message(cast(str, msg[\"content\"]))\n",
    "                    )\n",
    "                else:\n",
    "                    messages.append(\n",
    "                        MessageParam(\n",
    "                            role=\"assistant\",\n",
    "                            content=[\n",
    "                                TextBlockParam(\n",
    "                                    type=\"text\", text=cast(str, msg[\"content\"])\n",
    "                                )\n",
    "                            ],\n",
    "                        )\n",
    "                    )\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported role: {role}\")\n",
    "\n",
    "        return messages\n",
    "\n",
    "    def _create_thinking_message(self, text: str) -> MessageParam:\n",
    "        content: List[Union[TextBlockParam, ThinkingBlockParam]] = []\n",
    "\n",
    "        thinking = extract_tag(text, \"thinking\")\n",
    "        signature = extract_tag(text, \"signature\")\n",
    "\n",
    "        if thinking:\n",
    "            content.append(\n",
    "                ThinkingBlockParam(\n",
    "                    type=\"thinking\",\n",
    "                    thinking=thinking.strip(),\n",
    "                    signature=signature.strip() if signature else \"\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        clean_text = text\n",
    "        if thinking:\n",
    "            clean_text = clean_text.replace(f\"<thinking>{thinking}</thinking>\", \"\")\n",
    "        if signature:\n",
    "            clean_text = clean_text.replace(f\"<signature>{signature}</signature>\", \"\")\n",
    "\n",
    "        if clean_text.strip():\n",
    "            content.append(TextBlockParam(type=\"text\", text=clean_text.strip()))\n",
    "\n",
    "        return MessageParam(role=\"assistant\", content=content)\n",
    "\n",
    "    def _handle_stream(\n",
    "        self,\n",
    "        stream: anthropic.Stream[anthropic.MessageStreamEvent],\n",
    "    ) -> Iterator[Optional[str]]:\n",
    "        def f() -> Iterator[Optional[str]]:\n",
    "            for chunk in stream:\n",
    "                if chunk.type == \"content_block_delta\":\n",
    "                    if hasattr(chunk.delta, \"text\"):\n",
    "                        yield chunk.delta.text\n",
    "                elif chunk.type == \"message_stop\":\n",
    "                    yield None\n",
    "\n",
    "        return f()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Public APIs\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def chat(\n",
    "        self,\n",
    "        chat: Sequence[Message],\n",
    "        **kwargs: Any,\n",
    "    ) -> Union[str, Iterator[Optional[str]]]:\n",
    "        tmp_kwargs, thinking_enabled = self._setup_chat_kwargs(kwargs)\n",
    "        messages = self._convert_messages(chat, thinking_enabled, **kwargs)\n",
    "\n",
    "        response = self.client.messages.create(\n",
    "            model=self.model_name,\n",
    "            messages=messages,\n",
    "            **tmp_kwargs,\n",
    "        )\n",
    "\n",
    "        if tmp_kwargs.get(\"stream\", False):\n",
    "            return self._handle_stream(\n",
    "                cast(anthropic.Stream[anthropic.MessageStreamEvent], response)\n",
    "            )\n",
    "\n",
    "        # non-streaming\n",
    "        msg = cast(anthropic.types.Message, response)\n",
    "        return cast(anthropic.types.TextBlock, msg.content[-1]).text\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        media: Optional[Sequence[Union[str, Path]]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Union[str, Iterator[Optional[str]]]:\n",
    "        content: List[Union[TextBlockParam, ImageBlockParam]] = [\n",
    "            TextBlockParam(type=\"text\", text=prompt)\n",
    "        ]\n",
    "\n",
    "        if media:\n",
    "            for m in media:\n",
    "                resize = kwargs.get(\"resize\", self.image_size)\n",
    "                encoded = encode_media(cast(str, m), resize=resize)\n",
    "\n",
    "                content.append(\n",
    "                    ImageBlockParam(\n",
    "                        type=\"image\",\n",
    "                        source={\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/png\",\n",
    "                            \"data\": encoded,\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        tmp_kwargs = self.kwargs | kwargs\n",
    "        response = self.client.messages.create(\n",
    "            model=self.model_name,\n",
    "            messages=[MessageParam(role=\"user\", content=content)],\n",
    "            **tmp_kwargs,\n",
    "        )\n",
    "\n",
    "        if tmp_kwargs.get(\"stream\", False):\n",
    "\n",
    "            def f() -> Iterator[Optional[str]]:\n",
    "                for chunk in response:\n",
    "                    if chunk.type == \"content_block_delta\":\n",
    "                        yield chunk.delta.text\n",
    "                    elif chunk.type == \"message_stop\":\n",
    "                        yield None\n",
    "\n",
    "            return f()\n",
    "\n",
    "        return cast(anthropic.types.TextBlock, response.content[-1]).text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3961d85b",
   "metadata": {},
   "source": [
    "planner 역할\n",
    "\n",
    "Plan: LLM - 계획 만들게  -> LLM이 만든 코드 실행 ->\n",
    "<br> Critic: 관찰값 -> 다시 LLM -> \n",
    "<br> Summerize: plan + instruction + code 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c77088e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1617616477.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m# Top - Down\u001b[39m\n                ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class Planner():\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        # 호출할 VLM 지정   \n",
    "        planner: Optional[LMM] = None,\n",
    "        critic: Optional[LMM] = None,\n",
    "        summerizer: Optional[LMM] = None;\n",
    "        verbose: bool = False,\n",
    "    ):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6935d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coder():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9e132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planner 호출\n",
    "cfg = Config()\n",
    "\n",
    "planner = Planner(\n",
    "    planner=cfg.create_planner(),\n",
    "    summarizer=cfg.create_summarizer(),\n",
    "    critic=cfg.create_critic(),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "with CodeInterpreterFactory.new_instance(None) as code_interpreter:\n",
    "    plan_context = planner.generate_plan(\n",
    "        int_chat,\n",
    "        max_steps=10,                 # 필요하면 조절\n",
    "        code_interpreter=code_interpreter,\n",
    "    )\n",
    "\n",
    "print(\"\\nPlan context object type:\", type(plan_context))\n",
    "print(\"\\nPlan context dump (if available):\")\n",
    "try:\n",
    "    print(plan_context.model_dump())\n",
    "except Exception:\n",
    "    print(plan_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0643f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# planner tools\n",
    "def suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a1e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e88344",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
